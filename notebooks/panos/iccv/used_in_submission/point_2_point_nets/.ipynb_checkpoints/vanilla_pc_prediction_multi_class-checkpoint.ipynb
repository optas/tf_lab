{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/orions4-zfs/projects/lins2/Panos_Space/Git_Repos/geo_tool/solids/mesh.py:26: UserWarning: Mayavi library was not found. Some graphics utilities will be disabled.\n",
      "  warnings.warn('Mayavi library was not found. Some graphics utilities will be disabled.')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import hmean\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from tf_lab.fundamentals.utils import set_visible_GPUs, reset_tf_graph\n",
    "\n",
    "import tf_lab.point_clouds.in_out as pio\n",
    "from tf_lab.point_clouds.in_out import PointCloudDataSet, write_model_ids_of_datasets\n",
    "from tf_lab.point_clouds.point_net_ae import PointNetAutoEncoder\n",
    "from tf_lab.point_clouds.autoencoder import Configuration as Conf\n",
    "import tf_lab.point_clouds.encoders_decoders as enc_dec\n",
    "\n",
    "\n",
    "import tf_lab.autopredictors.scripts.virt_scan_data as vscan\n",
    "\n",
    "from tf_lab.autopredictors.scripts.helper import shape_net_category_to_synth_id, points_extension, \\\n",
    "                                                 shape_net_core_synth_id_to_category\n",
    "\n",
    "\n",
    "from tf_lab.autopredictors.plotting import plot_original_pclouds_vs_reconstructed, \\\n",
    "                                           plot_train_val_test_curves, plot_reconstructions_at_epoch\n",
    "        \n",
    "from tf_lab.autopredictors.evaluate import eval_model, read_saved_epochs, accuracy_of_completion, \\\n",
    "                                           coverage_of_completion, save_reconstructions, \\\n",
    "                                           save_pc_prediction_stats, save_stats_of_multi_class_experiments, \\\n",
    "                                           paper_pc_completion_experiment_id_best_epoch\n",
    "                                                  \n",
    "from tf_lab.autopredictors.exploration import latent_embedding_of_entire_dataset\n",
    "\n",
    "from general_tools.in_out.basics import create_dir, delete_files_in_directory, files_in_subdirs\n",
    "from general_tools.simpletons import select_first_last_and_k\n",
    "from geo_tool import Point_Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "GPU = 0\n",
    "loss = 'chamfer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if loss == 'emd':\n",
    "    exp_counter = '9'\n",
    "else:\n",
    "    exp_counter = '10'\n",
    "\n",
    "load_model_conf = True\n",
    "do_training = False\n",
    "do_evaluation = False\n",
    "\n",
    "incomplete_n_samples = 2048\n",
    "complete_n_samples = 4096\n",
    "\n",
    "val_percent = .10\n",
    "dropout_keep_prob = .8\n",
    "seed = 42\n",
    "\n",
    "experiment_name = exp_counter + '_all_classes_' + str(incomplete_n_samples) + '_' \\\n",
    "                  + str(complete_n_samples) + 'pts_' + loss\n",
    "\n",
    "top_data_dir = '/orions4-zfs/projects/lins2/Panos_Space/DATA/'\n",
    "\n",
    "n_input = [incomplete_n_samples, 3]\n",
    "n_output = [complete_n_samples, 3] \n",
    "\n",
    "train_dir = osp.join(top_data_dir, 'OUT/models/incomplete_pclouds/paper_vanilla_vscan')\n",
    "train_dir = osp.join(train_dir, experiment_name)\n",
    "create_dir(train_dir)\n",
    "\n",
    "max_training_epochs = 100    \n",
    "max_evaluation_epochs = max_training_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4045 files containing complete point clouds were found.\n",
      "19800 incomplete point clouds were loaded.\n",
      "4470 incomplete point clouds were loaded.\n"
     ]
    }
   ],
   "source": [
    "class_to_syn_id = shape_net_category_to_synth_id()\n",
    "all_classes = vscan.all_classes\n",
    "n_threads = 50\n",
    "\n",
    "first = class_to_syn_id[all_classes[0]]\n",
    "train_data, val_data, test_data = vscan.load_train_val_test_vscan_paper(first, n_threads,\\\n",
    "                                                                        complete_n_samples=complete_n_samples,\\\n",
    "                                                                        incomplete_n_samples=incomplete_n_samples,\n",
    "                                                                        val_percent=val_percent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4045 files containing complete point clouds were found.\n",
      "19800 incomplete point clouds were loaded.\n",
      "4470 incomplete point clouds were loaded.\n",
      "1572 files containing complete point clouds were found.\n",
      "7800 incomplete point clouds were loaded.\n",
      "1632 incomplete point clouds were loaded.\n",
      "7497 files containing complete point clouds were found.\n",
      "29640 incomplete point clouds were loaded.\n",
      "5922 incomplete point clouds were loaded.\n",
      "6778 files containing complete point clouds were found.\n",
      "30000 incomplete point clouds were loaded.\n",
      "6000 incomplete point clouds were loaded.\n",
      "2318 files containing complete point clouds were found.\n",
      "11100 incomplete point clouds were loaded.\n",
      "2808 incomplete point clouds were loaded.\n",
      "3173 files containing complete point clouds were found.\n",
      "15600 incomplete point clouds were loaded.\n",
      "3438 incomplete point clouds were loaded.\n",
      "8509 files containing complete point clouds were found.\n",
      "30000 incomplete point clouds were loaded.\n",
      "6000 incomplete point clouds were loaded.\n",
      "1939 files containing complete point clouds were found.\n",
      "9600 incomplete point clouds were loaded.\n",
      "2034 incomplete point clouds were loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load Data of All Classes (Takes Time.)\n",
    "class_to_syn_id = shape_net_category_to_synth_id()\n",
    "all_classes = vscan.all_classes\n",
    "n_threads = 50\n",
    "\n",
    "first = class_to_syn_id[all_classes[0]]\n",
    "train_data, val_data, test_data = vscan.load_train_val_test_vscan_paper(first, n_threads,\\\n",
    "                                                                        complete_n_samples=complete_n_samples,\\\n",
    "                                                                        incomplete_n_samples=incomplete_n_samples,\n",
    "                                                                        val_percent=val_percent)\n",
    "for model_class in vscan.all_classes[1:]:\n",
    "    class_syn_id = class_to_syn_id[model_class]\n",
    "    curr_train, curr_val, curr_test = vscan.load_train_val_test_vscan_paper(class_syn_id, n_threads,\\\n",
    "                                                              complete_n_samples=complete_n_samples,\\\n",
    "                                                              incomplete_n_samples=incomplete_n_samples,\\\n",
    "                                                              val_percent=val_percent)\n",
    "    train_data.merge(curr_train)\n",
    "    test_data.merge(curr_test)\n",
    "    val_data.merge(curr_val)\n",
    "\n",
    "train_data.shuffle_data();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Verification we didn't mix train-test-val data.\n",
    "\n",
    "# print train_data.num_examples + val_data.num_examples\n",
    "# print test_data.num_examples\n",
    "\n",
    "# tr = train_data.full_epoch_data(shuffle=False)\n",
    "# va = val_data.full_epoch_data(shuffle=False)\n",
    "# te = test_data.full_epoch_data(shuffle=False)\n",
    "\n",
    "# train_set = set([i[:-6] for i in tr[1]])\n",
    "# val_set = set([i[:-6] for i in va[1]])\n",
    "# test_set = set([i[:-6] for i in te[1]])\n",
    "\n",
    "# c1 = len(test_set.intersection(train_set)) == 0\n",
    "# c2 = len(test_set.intersection(val_set)) == 0\n",
    "# c3 = len(train_set.intersection(val_set)) == 0\n",
    "\n",
    "# assert(c1 and c2 and c3)\n",
    "\n",
    "# pp = test_data.next_batch(1)\n",
    "# pinc = pp[2].reshape(incomplete_n_samples, 3)\n",
    "# pcom = pp[0].reshape(complete_n_samples, 3)\n",
    "        \n",
    "# score1 = accuracy_of_completion(pinc, pcom, 0.02, ret_dists=False)\n",
    "# print score1\n",
    "# score2, c2 = coverage_of_completion(pcom, pinc, 0.02, ret_dists=True)\n",
    "\n",
    "# Point_Cloud(points=pinc).plot();\n",
    "# Point_Cloud(points=pcom).plot(c=c2);\n",
    "# print pp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              allow_gpu_growth: False\n",
      "                    batch_size: 50\n",
      "                 consistent_io: False\n",
      "                         debug: False\n",
      "                       decoder: decoder_with_fc_only\n",
      "                  decoder_args: {'non_linearity': <function relu at 0x7fe3c2ad9e60>, 'layer_sizes': [1024, 12288]}\n",
      "                       encoder: encoder_with_convs_and_symmetry\n",
      "                  encoder_args: {'dropout_prob': 0.8}\n",
      "               experiment_name: 10_all_classes_2048_4096pts_chamfer\n",
      "                 gauss_augment: None\n",
      "                  is_denoising: True\n",
      "               latent_vs_recon: 1.0\n",
      "                 learning_rate: 0.0005\n",
      "                          loss: chamfer\n",
      "             loss_display_step: 1\n",
      "                       n_input: [2048, 3]\n",
      "                      n_output: [4096, 3]\n",
      "                           n_z: None\n",
      "             saver_max_to_keep: None\n",
      "                    saver_step: 1\n",
      "                     train_dir: /orions4-zfs/projects/lins2/Panos_Space/DATA/OUT/models/incomplete_pclouds/paper_vanilla_vscan/10_all_classes_2048_4096pts_chamfer\n",
      "               training_epochs: 300\n",
      "                      z_rotate: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(seed)\n",
    "\n",
    "if load_model_conf:\n",
    "    conf = Conf.load(osp.join(train_dir, 'configuration'))\n",
    "    print conf\n",
    "else:\n",
    "    decoder_args = {'layer_sizes': [1024, np.prod(n_output)],\n",
    "                    'non_linearity': tf.nn.relu\n",
    "                   }\n",
    "\n",
    "    encoder_args = {'dropout_prob': dropout_keep_prob}\n",
    "    \n",
    "    conf = Conf(\n",
    "                n_input = n_input,\n",
    "                n_output = n_output,\n",
    "                denoising = True,\n",
    "                training_epochs = max_training_epochs,\n",
    "                batch_size = 50,\n",
    "                loss = loss,\n",
    "                train_dir = train_dir,\n",
    "                loss_display_step = 1,\n",
    "                saver_step = 1,\n",
    "                learning_rate = 0.0005,\n",
    "                encoder = enc_dec.encoder_with_convs_and_symmetry,\n",
    "                encoder_args = encoder_args,\n",
    "                decoder = enc_dec.decoder_with_fc_only,\n",
    "                decoder_args = decoder_args\n",
    "               )\n",
    "    \n",
    "    conf.allow_gpu_growth = False\n",
    "    conf.experiment_name = experiment_name\n",
    "    conf.save(osp.join(conf.train_dir, 'configuration'))\n",
    "\n",
    "conf.consistent_io = None    \n",
    "reset_tf_graph()\n",
    "set_visible_GPUs([GPU])\n",
    "ae = PointNetAutoEncoder(experiment_name, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if do_training:\n",
    "    training_stats = []\n",
    "    training_stats.append(ae.train(train_data, conf))    \n",
    "    with open(osp.join(conf.train_dir, 'train_stats.txt'), 'a') as fout:\n",
    "        np.savetxt(fout, np.array(training_stats)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if do_evaluation:    \n",
    "    # Pick the epoch that minimizes the loss on the validation dataset.\n",
    "    saved_epochs = np.array(read_saved_epochs(conf.train_dir))\n",
    "    allowable_epochs = saved_epochs[saved_epochs <= max_evaluation_epochs]\n",
    "    val_stats = eval_model(ae, conf, val_data, epochs=allowable_epochs, verbose=True)\n",
    "    val_loss = np.min(val_stats[:,1])\n",
    "    best_epoch = int(val_stats[np.argmin(val_stats[:,1]), 0])\n",
    "    print 'Best epoch = %d.' % (best_epoch,) \n",
    "        \n",
    "    ae.restore_model(conf.train_dir, best_epoch)\n",
    "    top_save_dir = osp.join(conf.train_dir, 'output', 'epoch_' + str(best_epoch))\n",
    "    save_dir = osp.join(top_save_dir, 'test_predictions')\n",
    "    test_recon, test_loss, test_feed, test_ids, test_gt = ae.evaluate(test_data, conf)\n",
    "    save_reconstructions(save_dir, test_recon, test_gt, test_feed, test_ids) # save ply files of test data.    \n",
    "    train_loss = ae.evaluate(train_data, conf)[1]\n",
    "    \n",
    "    # Report Accuracy and Coverage of test data.\n",
    "    n_examples = len(test_recon)\n",
    "    pred_scores = np.zeros((n_examples, 2))\n",
    "    for i in xrange(n_examples):\n",
    "        gt = test_gt[i]\n",
    "        pred = test_recon[i] \n",
    "        pred_scores[i, 0] = accuracy_of_completion(pred, gt, thres=0.02, ret_dists=False)\n",
    "        pred_scores[i, 1] = coverage_of_completion(gt, pred, thres=0.02, ret_dists=False)\n",
    "    \n",
    "    print 'Test Median-Accuracy-Coverage:', np.median(pred_scores[:, 0]), np.median(pred_scores[:, 1])\n",
    "    \n",
    "    save_pc_prediction_stats(osp.join(top_save_dir, 'detailed_stats.txt'), test_ids, pred_scores)\n",
    "    save_stats_of_multi_class_experiments(osp.join(top_save_dir, 'class_stats.txt'), test_ids, pred_scores)\n",
    "    \n",
    "    with open(osp.join(top_save_dir, 'stats.txt'), 'w') as fout:\n",
    "        fout.write('Best Validation Epoch = %d\\n' % (best_epoch))\n",
    "        fout.write('Validation loss = %f\\n' % (val_loss))\n",
    "        fout.write('Train loss = %f\\n' % (train_loss))\n",
    "        fout.write('Test loss = %f\\n' % (test_loss))\n",
    "        fout.write('Gen. Error (abs, per) = %f %f\\n' % (abs(test_loss-train_loss),  abs(test_loss-train_loss) / train_loss ))\n",
    "        fout.write('Test Median-Accuracy-Coverage = %f %f\\n' % (np.median(pred_scores[:, 0]), np.median(pred_scores[:, 1])))\n",
    "        fout.write('Test Median Harmonic Mean = %f' % (np.median(hmean(pred_scores, 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Latent codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class_type = 'all_classes'\n",
    "exp_counter, best_epoch = paper_pc_completion_experiment_id_best_epoch(class_type, loss)\n",
    "ae.restore_model(conf.train_dir, best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feed, latent_emb, ids = latent_embedding_of_entire_dataset(test_data, ae, conf, feed_original=False, apply_augmentation=False)\n",
    "\n",
    "tokens = [i.split('.') for i in ids]\n",
    "test_syn_ids = np.array([token[0] for token in tokens], dtype=object)\n",
    "test_model_names = np.array([token[1] for token in tokens], dtype=object)\n",
    "test_scan_ids = np.array([token[2] for token in tokens], dtype=object)\n",
    "\n",
    "# model = TSNE(n_components=2, random_state=seed, init='pca', verbose=True);\n",
    "# tsne_emb = model.fit_transform(latent_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from general_tools.in_out.basics import pickle_data, unpickle_data\n",
    "pickle_data('%s-tsne-data.pkl' % (loss,), ids, latent_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tf_lab.autopredictors.scripts.helper import syn_id_to_class_id_dict\n",
    "from general_tools.arrays.transform import make_contiguous\n",
    "\n",
    "syn_id_to_cat = shape_net_core_synth_id_to_category\n",
    "syn_id_to_int = syn_id_to_class_id_dict()\n",
    "c_int = [syn_id_to_int[i] for i in test_syn_ids]\n",
    "\n",
    "uvalues = np.unique(c_int)\n",
    "inv_map = {v: k for k, v in syn_id_to_int.iteritems()}\n",
    "color_bar_labels = [syn_id_to_cat[inv_map[u]] for u in uvalues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 800 / 800\n",
      "[t-SNE] Mean sigma: 0.526402\n",
      "[t-SNE] KL divergence after 100 iterations with early exaggeration: 0.653676\n",
      "[t-SNE] Error after 175 iterations: 0.653676\n"
     ]
    }
   ],
   "source": [
    "samples_per_class = 100\n",
    "selected = []\n",
    "for class_id in np.unique(test_syn_ids):\n",
    "    in_class = np.where(test_syn_ids == class_id)[0]\n",
    "    selected += np.ndarray.tolist(np.random.choice(in_class, samples_per_class, replace=False))\n",
    "\n",
    "\n",
    "exemplars = np.array(selected)\n",
    "latent_emb_small = latent_emb[exemplars]\n",
    "model = TSNE(n_components=2, random_state=seed, init='pca', verbose=True);\n",
    "tsne_emb_small = model.fit_transform(latent_emb_small)\n",
    "c = np.array(c_int)[exemplars]\n",
    "c = make_contiguous(c)\n",
    "plt.scatter(tsne_emb_small[:,0], tsne_emb_small[:,1], c=c)\n",
    "cbar = plt.colorbar(ticks=range(0,9))\n",
    "cbar.set_ticklabels(color_bar_labels)\n",
    "plt.grid()\n",
    "plt.suptitle('TSNE of Test Data - %s Loss.' % (loss, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
