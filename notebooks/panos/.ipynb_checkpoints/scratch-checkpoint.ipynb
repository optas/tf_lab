{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Cloud AutoEncoders \"Experimental\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from general_tools.in_out import create_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '/Users/optas/DATA/Point_Clouds/Shape_Net_Core/3000/no_segmentations/03001627/'\n",
    "LOG_PATH = '/Users/optas/DATA/Neural_Nets/Train_Log/Point_Cloud_AE/'\n",
    "MODEL_PATH = '/Users/optas/DATA/Neural_Nets/Models/Point_Cloud_AE'\n",
    "create_dir(LOG_PATH);\n",
    "create_dir(MODEL_PATH);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tf_lab.point_clouds import point_cloud_ae as pae\n",
    "from tf_lab.point_clouds.point_cloud_ae import Configuration as ae_conf\n",
    "import tf_lab.point_clouds.in_out as pio\n",
    "from tf_lab.fundamentals.loss import Loss\n",
    "from tf_lab.fundamentals.inspect import hist_summary_of_trainable, sparsity_summary_of_trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = ae_conf(n_points=3000, training_epochs=100, batch_size=20)\n",
    "\n",
    "in_signal, gt_signal = pio.in_out_placeholders(config)\n",
    "\n",
    "ae_model = pae.autoencoder_with_fcs_only(in_signal, config)\n",
    "\n",
    "loss = Loss.l2_loss(ae_model, gt_signal)\n",
    "\n",
    "# optimizer = tf.train.GradientDescentOptimizer(config.learning_rate).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=config.learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all_summaries = []\n",
    "# all_summaries.extend(hist_summary_of_trainable())\n",
    "# # summary_2 = sparsity_summary_of_trainable()\n",
    "# all_summaries.extend([tf.summary.scalar('Loss.', loss)])\n",
    "# all_summaries = tf.merge_summary(all_summaries)\n",
    "# train_writer = tf.train.SummaryWriter(LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6778 files containing  point clouds were found.\n"
     ]
    }
   ],
   "source": [
    "file_names = pio.load_filenames_of_input_data(DATA_PATH)\n",
    "all_pclouds = pio.load_crude_point_clouds(file_names=file_names)\n",
    "# pio.convert_data_to_tfrecord(file_names, DATA_PATH, 'chairs_3k', pio._convert_point_cloud_to_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6778 files containing  point clouds were found.\n"
     ]
    }
   ],
   "source": [
    "# # file_generator = pio.chunks(in_files, config.batch_size)\n",
    "# # with tf.Session() as sess:\n",
    "# num_epochs = 2\n",
    "# with tf.Graph().as_default():\n",
    "#     pcloud = input_point_cloud(filename, 2)\n",
    "#     init_op = tf.initialize_all_variables()    \n",
    "    \n",
    "#     with tf.Session() as sess:\n",
    "#         sess.run(init_op)\n",
    "#         filename_queue = tf.train.string_input_producer(file_names, num_epochs=num_epochs, shuffle=True)\n",
    "#         lala = sess.run(read_and_decode(filename_queue))\n",
    "\n",
    "\n",
    "#         # Start populating the filename queue.\n",
    "#         coord = tf.train.Coordinator()\n",
    "#         threads = tf.train.start_queue_runners(coord=coord)\n",
    "        \n",
    "#         try:\n",
    "#             while not coord.should_stop():        \n",
    "#         yo = sess.run([pcloud])\n",
    "    \n",
    "#         except tf.errors.OutOfRangeError:\n",
    "#               print('Done training for %d epochs, %d steps.' % (FLAGS.num_epochs, step))\n",
    "#         finally:            \n",
    "#             coord.request_stop()\n",
    "\n",
    "#         # Wait for threads to finish.\n",
    "#         coord.join(threads)\n",
    "#         sess.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def make_batches_for_epoch(file_names, batch_size, randomize=True):\n",
    "    batch_data = []\n",
    "    if randomize is True:\n",
    "        shuffle(file_names)\n",
    "    \n",
    "    for models_i in pio.chunks(file_names, batch_size):\n",
    "        if len(models_i) == batch_size:\n",
    "            batch_data.append(models_i)\n",
    "    return batch_data\n",
    "\n",
    "batch_data = make_batches_for_epoch(file_names, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "n_examples = len(file_names)\n",
    "loss_display_step = 1\n",
    "stats_display_step = 10\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.initialize_all_variables()        \n",
    "    sess.run(init_op)\n",
    "    for epoch in range(config.training_epochs):\n",
    "        avg_cost = 0.\n",
    "        n_batches = int(n_examples / config.batch_size)        \n",
    "        batch_data = make_batches_for_epoch(file_names, batch_size)\n",
    "    \n",
    "        start_time = time.time()\n",
    "        # Loop over all batches\n",
    "        for i in xrange(n_batches):\n",
    "        \n",
    "            batch_i = batch_data[i]\n",
    "            pclouds, _ = pio.load_crude_point_clouds(file_names=batch_i)\n",
    "            pclouds = np.array(pclouds)\n",
    "            feed_dict = {in_signal:pclouds, gt_signal:pclouds}    \n",
    "            _, cost = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "            avg_cost += cost / n_examples * batch_size\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        print duration\n",
    "        \n",
    "        # Display loss per epoch step\n",
    "        if epoch % loss_display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "            \n",
    "      \n",
    "\n",
    "                  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_and_decode(filename_queue):\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "    features = tf.parse_single_example(serialized_example,\\\n",
    "        features={'pcloud_raw' : tf.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "                  'model_name' : tf.VarLenFeature(dtype=tf.string),\n",
    "                  'class_name' : tf.VarLenFeature(dtype=tf.string)\n",
    "                 })\n",
    "\n",
    "    pcloud = tf.decode_raw(features['pcloud_raw'], tf.float32)\n",
    "    pcloud.set_shape([9000])\n",
    "    return pcloud, features['model_name'], features['class_name']\n",
    "\n",
    "\n",
    "\n",
    "def input_point_cloud(filename, batch_size, num_epochs=None):    \n",
    "\n",
    "    with tf.name_scope('input'):        \n",
    "        filename_queue = tf.train.string_input_producer([filename], num_epochs=num_epochs) #, shuffle=True)            \n",
    "        example, model_name, class_name = read_and_decode(filename_queue)\n",
    "\n",
    "        \n",
    "        min_after_dequeue = 10000\n",
    "        capacity = min_after_dequeue + 3 * batch_size\n",
    "\n",
    "        example_batch = tf.train.shuffle_batch(\\\n",
    "                                     [example, model_name], batch_size=batch_size, capacity=capacity,\\\n",
    "                                     min_after_dequeue=min_after_dequeue)\n",
    "\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09833967 -0.20970739 -0.31230238] [-0.16016431 -0.18467321 -0.35670862]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://github.com/tensorflow/models/blob/master/inception/inception/data/build_imagenet_data.py\n",
    "    \n",
    "# https://indico.io/blog/tensorflow-data-inputs-part1-placeholders-protobufs-queues/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRYING TO LOAD VERTEX QUALITY FROM MESHLAB\n",
    "\n",
    "from geo_tool.external_code.python_plyfile.plyfile import PlyData\n",
    "from geo_tool import Mesh\n",
    "import numpy as np\n",
    "\n",
    "test_file = '/Users/optas/Desktop/horse-gallop-21.off.ply'\n",
    "file_name =test_file\n",
    "def load_ply(file_name, with_faces=False):\n",
    "    ply_data = PlyData.read(file_name)\n",
    "    points = ply_data['vertex']\n",
    "    p_quality = points['quality']\n",
    "    points = [np.vstack([points['x'], points['y'], points['z']]).T, p_quality]\n",
    "    \n",
    "    if with_faces:\n",
    "        faces = np.vstack(ply_data['face']['vertex_indices'])\n",
    "        return points, faces\n",
    "    else:\n",
    "        return points\n",
    "\n",
    "# v, t = load_ply(test_file, with_faces=True)\n",
    "# Mesh(vertices=v[0], triangles=t).plot(vertex_function = v[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _float_feature(value):\n",
    "    '''Wrapper for inserting float features into Example proto.'''\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "import itertools\n",
    "def flatten(list_of_lists):\n",
    "    \"\"\"Flatten one level of nesting.\"\"\"\n",
    "    return itertools.chain.from_iterable(list_of_lists)\n",
    "\n",
    "\n",
    "def convert_pcloud_to_tfrecord(file_names, out_dir):\n",
    "    '''Converts the point-clouds listed to tfrecords.\n",
    "    '''    \n",
    "    \n",
    "    out_file = osp.join(out_dir, 'testing' + '.tfrecords')\n",
    "    writer = tf.python_io.TFRecordWriter(out_file)\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        pcloud_raw = pio.load_crude_point_cloud(file_name)        \n",
    "        model_name = osp.basename(file_name).split('_')[0]\n",
    "        synset = osp.split(file_name)[0].split(osp.sep)[-1]\n",
    "\n",
    "        pcloud_raw = pcloud_raw.tostring()\n",
    "                        \n",
    "#         pcloud_raw = pcloud_raw.flatten().tolist()\n",
    "#         out_file = osp.join(out_dir, model_name + '.tfrecords')\n",
    "#         print('Writing', out_file)        \n",
    "\n",
    "        # Construct an Example proto object.\n",
    "        example = tf.train.Example(\\\n",
    "            # Example contains a Features proto object.\n",
    "            features=tf.train.Features\\\n",
    "                # Features contains a map of string to Feature proto objects.\n",
    "                (feature={'pcloud_raw' : _bytes_feature(pcloud_raw),\n",
    "                          'model_name' : _bytes_feature(model_name),\n",
    "                          'class_name' : _bytes_feature(class_name)\n",
    "                         }))\n",
    "        \n",
    "#                 (feature={'pcloud_raw': _float_feature(pcloud_raw)}))        \n",
    "        # Use the proto object to serialize the example to a string.\n",
    "        serialized = example.SerializeToString()\n",
    "        # Write the serialized object to disk.\n",
    "        writer.write(serialized)        \n",
    "    writer.close()\n",
    "\n",
    "    \n",
    "file_names_ori = pio.load_filenames_of_input_data(DATA_PATH_ORIGINAL)\n",
    "file_names_ori = file_names_ori[:100]\n",
    "\n",
    "\n",
    "convert_pcloud_to_tfrecord(file_names_ori, '/Users/optas/Desktop/test3')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
