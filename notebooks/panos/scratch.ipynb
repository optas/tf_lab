{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Cloud AutoEncoders \"Experimental\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from general_tools.in_out import create_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_PATH_ORIGINAL = '/Users/optas/DATA/Point_Clouds/Shape_Net_Core/3000/no_segmentations/03001627/'\n",
    "DATA_PATH = '/Users/optas/Desktop/test3/'\n",
    "LOG_PATH = '/Users/optas/DATA/Neural_Nets/Train_Log/Point_Cloud_AE'\n",
    "MODEL_PATH = '/Users/optas/DATA/Neural_Nets/Models/Point_Cloud_AE'\n",
    "create_dir(LOG_PATH)\n",
    "create_dir(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUEUE_EPOCH_FRACTION = 0.1\n",
    "NUM_EPOCHS_PER_DECAY = 8          # Epochs after which learning rate decays.\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.\n",
    "INITIAL_LEARNING_RATE = 0.001     # Initial learning rate.\n",
    "\n",
    "#     keep_prob = 0.5\n",
    "#     stddev = 5e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tf_lab.point_clouds import point_cloud_ae as pae\n",
    "from tf_lab.point_clouds.point_cloud_ae import Configuration as ae_conf\n",
    "import tf_lab.point_clouds.in_out as pio\n",
    "from tf_lab.fundamentals.loss import Loss\n",
    "from tf_lab.fundamentals.inspect import hist_summary_of_trainable, sparsity_summary_of_trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable fc_1/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/Users/optas/Documents/Git_Repos/tf_lab/fundamentals/nn.py\", line 27, in _variable_with_weight_decay\n    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype, trainable=trainable)\n  File \"/Users/optas/Documents/Git_Repos/tf_lab/fundamentals/layers.py\", line 43, in fully_connected_layer\n    weights = _variable_with_weight_decay('weights', [dim, out_dim], stddev=stddev, wd=wd)\n  File \"/Users/optas/Documents/Git_Repos/tf_lab/point_clouds/point_cloud_ae.py\", line 27, in autoencoder_with_fcs_only\n    layer = fully_connected_layer(in_signal, hs[0], stddev=0.01, name='fc_1')\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4f4faa4f2f72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mae_conf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0min_signal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_signal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_out_placeholders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoencoder_with_fcs_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_signal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_signal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINITIAL_LEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/optas/Documents/Git_Repos/tf_lab/point_clouds/point_cloud_ae.pyc\u001b[0m in \u001b[0;36mautoencoder_with_fcs_only\u001b[0;34m(in_signal, configuration)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0min_signal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_signal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfully_connected_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_signal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fc_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfully_connected_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fc_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfully_connected_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fc_3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/optas/Documents/Git_Repos/tf_lab/fundamentals/layers.pyc\u001b[0m in \u001b[0;36mfully_connected_layer\u001b[0;34m(in_layer, out_dim, stddev, wd, init_bias, name)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0min_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_flat_batch_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_variable_with_weight_decay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_bias_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mout_signal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_out'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/optas/Documents/Git_Repos/tf_lab/fundamentals/nn.pyc\u001b[0m in \u001b[0;36m_variable_with_weight_decay\u001b[0;34m(name, shape, stddev, wd, dtype, trainable)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwd\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/optas/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m   1020\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/optas/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    847\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m           custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/Users/optas/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    343\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m           validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/Users/optas/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[1;32m    328\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m           caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/optas/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[1;32m    631\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 633\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    634\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable fc_1/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/Users/optas/Documents/Git_Repos/tf_lab/fundamentals/nn.py\", line 27, in _variable_with_weight_decay\n    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype, trainable=trainable)\n  File \"/Users/optas/Documents/Git_Repos/tf_lab/fundamentals/layers.py\", line 43, in fully_connected_layer\n    weights = _variable_with_weight_decay('weights', [dim, out_dim], stddev=stddev, wd=wd)\n  File \"/Users/optas/Documents/Git_Repos/tf_lab/point_clouds/point_cloud_ae.py\", line 27, in autoencoder_with_fcs_only\n    layer = fully_connected_layer(in_signal, hs[0], stddev=0.01, name='fc_1')\n"
     ]
    }
   ],
   "source": [
    "config = ae_conf()\n",
    "in_signal, gt_signal = pio.in_out_placeholders(config)\n",
    "model = pae.autoencoder_with_fcs_only(in_signal, config)\n",
    "loss = Loss.l2_loss(model, gt_signal)\n",
    "optimizer = tf.train.GradientDescentOptimizer(INITIAL_LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_summaries = []\n",
    "all_summaries.extend(hist_summary_of_trainable())\n",
    "# summary_2 = sparsity_summary_of_trainable()\n",
    "all_summaries.extend([tf.summary.scalar('Loss.', loss)])\n",
    "all_summaries = tf.merge_summary(all_summaries)\n",
    "train_writer = tf.train.SummaryWriter(LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6778 files containing  point clouds were found.\n"
     ]
    }
   ],
   "source": [
    "def _float_feature(value):\n",
    "    '''Wrapper for inserting float features into Example proto.'''\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "import itertools\n",
    "def flatten(list_of_lists):\n",
    "    \"\"\"Flatten one level of nesting.\"\"\"\n",
    "    return itertools.chain.from_iterable(list_of_lists)\n",
    "\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "\n",
    "def convert_pcloud_to_tfrecord(file_names, out_dir):\n",
    "    '''Converts the point-clouds listed to tfrecords.\n",
    "    '''    \n",
    "    \n",
    "    out_file = osp.join(out_dir, 'testing' + '.tfrecords')\n",
    "    writer = tf.python_io.TFRecordWriter(out_file)\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        pcloud_raw = pio.load_crude_point_cloud(file_name)        \n",
    "        model_name = osp.basename(file_name).split('_')[0]\n",
    "        synset = osp.split(file_name)[0].split(osp.sep)[-1]\n",
    "\n",
    "        pcloud_raw = pcloud_raw.tostring()\n",
    "                        \n",
    "#         pcloud_raw = pcloud_raw.flatten().tolist()\n",
    "#         out_file = osp.join(out_dir, model_name + '.tfrecords')\n",
    "#         print('Writing', out_file)        \n",
    "\n",
    "        # Construct an Example proto object.\n",
    "        example = tf.train.Example(\\\n",
    "            # Example contains a Features proto object.\n",
    "            features=tf.train.Features\\\n",
    "                # Features contains a map of string to Feature proto objects.\n",
    "                (feature={'pcloud_raw' : _bytes_feature(pcloud_raw),\n",
    "                          'model_name' : _bytes_feature(model_name),\n",
    "                          'class_name' : _bytes_feature(class_name)\n",
    "                         }))\n",
    "        \n",
    "#                 (feature={'pcloud_raw': _float_feature(pcloud_raw)}))        \n",
    "        # Use the proto object to serialize the example to a string.\n",
    "        serialized = example.SerializeToString()\n",
    "        # Write the serialized object to disk.\n",
    "        writer.write(serialized)        \n",
    "    writer.close()\n",
    "\n",
    "    \n",
    "file_names_ori = pio.load_filenames_of_input_data(DATA_PATH_ORIGINAL)\n",
    "file_names_ori = file_names_ori[:100]\n",
    "\n",
    "\n",
    "convert_pcloud_to_tfrecord(file_names_ori, '/Users/optas/Desktop/test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['02946921', '04256520', '02933112', '03207941', '03624134', '03948459', '04225987', '03761084', '03691459', '03710193', '02747177', '02808440', '02843684', '03938244', '03467517', '03797390', '03928116', '03325088', '03211117', '02801938', '04090263', '03593526', '02958343', '03513137', '02954340', '02773838', '04074963', '03790512', '03636649', '02880940', '04379243', '04099429', '03085013', '02924116', '02691156', '02942699', '02828884', '03337140', '03001627', '04330267', '02834778', '02871439', '02818832', '03261776', '03991062', '02876657', '03759954', '04004475', '03642806', '03046257']\n"
     ]
    }
   ],
   "source": [
    "# from autopredictors.scripts.helper import shape_net_core_synth_id_to_category\n",
    "# print shape_net_core_synth_id_to_category().keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_and_decode(filename_queue):\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "    features = tf.parse_single_example(serialized_example,\\\n",
    "        features={'pcloud_raw' : tf.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "                  'model_name' : tf.VarLenFeature(dtype=tf.string),\n",
    "                  'class_name' : tf.VarLenFeature(dtype=tf.string)\n",
    "                 })\n",
    "\n",
    "    pcloud = tf.decode_raw(features['pcloud_raw'], tf.float32)\n",
    "    pcloud.set_shape([9000])\n",
    "    return pcloud, features['model_name'], features['class_name']\n",
    "\n",
    "\n",
    "\n",
    "def input_point_cloud(filename, batch_size, num_epochs=None):    \n",
    "\n",
    "    with tf.name_scope('input'):        \n",
    "        filename_queue = tf.train.string_input_producer([filename], num_epochs=num_epochs) #, shuffle=True)            \n",
    "        example, model_name, class_name = read_and_decode(filename_queue)\n",
    "\n",
    "        \n",
    "        min_after_dequeue = 10000\n",
    "        capacity = min_after_dequeue + 3 * batch_size\n",
    "\n",
    "        example_batch = tf.train.shuffle_batch(\\\n",
    "                                     [example, model_name], batch_size=batch_size, capacity=capacity,\\\n",
    "                                     min_after_dequeue=min_after_dequeue)\n",
    "\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "tf_rec_extension = '.tfrecords'\n",
    "file_names = glob.glob(osp.join(DATA_PATH, '*' + tf_rec_extension))\n",
    "pc_len = len(file_names)\n",
    "total_steps = (pc_len // config.batch_size) * config.training_epochs\n",
    "print total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# file_generator = pio.chunks(in_files, config.batch_size)\n",
    "# with tf.Session() as sess:\n",
    "\n",
    "filename = '/Users/optas/Desktop/test3/testing.tfrecords'\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    pcloud = input_point_cloud(filename, 2)\n",
    "    init_op = tf.initialize_all_variables()    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)    \n",
    "        # Start populating the filename queue.\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        \n",
    "#         try:\n",
    "#             while not coord.should_stop():        \n",
    "        yo = sess.run([pcloud])\n",
    "    \n",
    "#         except tf.errors.OutOfRangeError:\n",
    "#               print('Done training for %d epochs, %d steps.' % (FLAGS.num_epochs, step))\n",
    "#         finally:\n",
    "            # When done, ask the threads to stop.\n",
    "        coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        sess.close()\n",
    "        \n",
    "\n",
    "#         filename_queue = tf.train.string_input_producer(file_names, num_epochs=None, shuffle=True)\n",
    "#         lala = sess.run(read_and_decode(filename_queue))\n",
    "#         print lala[0]\n",
    "    \n",
    "#     init_op = tf.initialize_all_variables()\n",
    "    \n",
    "#     coord.request_stop()\n",
    "#     coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uno = yo[0][0][0]\n",
    "duo = yo[0][1].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yea =  osp.join(DATA_PATH_ORIGINAL, duo + '_pts.txt' )\n",
    "pcloud_raw = pio.load_crude_point_cloud(yea)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09833967 -0.20970739 -0.31230238] [-0.16016431 -0.18467321 -0.35670862]\n"
     ]
    }
   ],
   "source": [
    "pcloud_raw\n",
    "b = uno.reshape([3000, 3])\n",
    "print b[0,:], pcloud_raw[0,:]\n",
    "\n",
    "# reshape[3000, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "https://github.com/tensorflow/models/blob/master/inception/inception/data/build_imagenet_data.py\n",
    "    \n",
    "# https://indico.io/blog/tensorflow-data-inputs-part1-placeholders-protobufs-queues/\n",
    "\n",
    "#     for step in xrange(total_steps):\n",
    "# #         pclouds, _ = pio.load_crude_point_clouds(file_names=file_generator.next())\n",
    "# #         pclouds = np.array(pclouds)\n",
    "\n",
    "#         pclouds = input_pipeline(file_names, config.batch_size)\n",
    "#         print pclouds\n",
    "        \n",
    "# #         feed_dict = {in_signal:pclouds, gt_signal:pclouds}    \n",
    "# #         _, l, s = sess.run([optimizer, loss, all_summaries], feed_dict=feed_dict)\n",
    "# #         print l\n",
    "# #         train_writer.add_summary(s, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRYING TO LOAD VERTEX QUALITY FROM MESHLAB\n",
    "\n",
    "from geo_tool.external_code.python_plyfile.plyfile import PlyData\n",
    "from geo_tool import Mesh\n",
    "import numpy as np\n",
    "\n",
    "test_file = '/Users/optas/Desktop/horse-gallop-21.off.ply'\n",
    "file_name =test_file\n",
    "def load_ply(file_name, with_faces=False):\n",
    "    ply_data = PlyData.read(file_name)\n",
    "    points = ply_data['vertex']\n",
    "    p_quality = points['quality']\n",
    "    points = [np.vstack([points['x'], points['y'], points['z']]).T, p_quality]\n",
    "    \n",
    "    if with_faces:\n",
    "        faces = np.vstack(ply_data['face']['vertex_indices'])\n",
    "        return points, faces\n",
    "    else:\n",
    "        return points\n",
    "\n",
    "# v, t = load_ply(test_file, with_faces=True)\n",
    "# Mesh(vertices=v[0], triangles=t).plot(vertex_function = v[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
