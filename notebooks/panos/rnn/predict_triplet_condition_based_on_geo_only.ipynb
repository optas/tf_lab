{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picking GPU 3\n"
     ]
    }
   ],
   "source": [
    "from general_tools.notebook.gpu_utils import setup_one_gpu\n",
    "GPU = 3\n",
    "setup_one_gpu(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import matplotlib.pylab as plt\n",
    "from pandas import DataFrame\n",
    "from general_tools.in_out.basics import files_in_subdirs\n",
    "from general_tools.in_out.basics import pickle_data, unpickle_data\n",
    "from tf_lab.point_clouds.encoders_decoders import decoder_with_fc_only\n",
    "from tf_lab.point_clouds.in_out import PointCloudDataSet, train_validate_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_triplets(triplets_file):\n",
    "    res = []\n",
    "    with open(triplets_file, 'r') as fin:\n",
    "        for line in fin:\n",
    "            tokens = line.split()\n",
    "            res.append([tokens[1], tokens[2], tokens[3]])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_data_of_single_condition(triplets_file, geo_features):\n",
    "    triplet_ids = read_triplets(triplets_file)\n",
    "    selected_feats = np.zeros(shape=(len(triplet_ids), geo_features.shape[1] * 3))\n",
    "    labels = np.zeros(shape=(len(triplet_ids), 3), dtype=np.int32)\n",
    "    \n",
    "    for i, tr in enumerate(triplet_ids):\n",
    "        geo_f = geo_features.loc[tr].as_matrix()\n",
    "        index = np.arange(3)\n",
    "        np.random.shuffle(index)\n",
    "        geo_f = geo_f[index, :]        \n",
    "        selected_feats[i] = np.hstack(geo_f)    \n",
    "        new_pos = np.where(index==0)[0][0]  # target is expexted to be first vector.\n",
    "        labels[i, new_pos] = 1 # one hot embedding\n",
    "        \n",
    "    return selected_feats, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "close_map_file = '/orions4-zfs/projects/lins2/Panos_Space/DATA/OUT/3d_neighbs_as_context/pilot_triplets/outlier_frac_30/closest_nn/data/model_names_of_triplets.txt'\n",
    "split_map_file = '/orions4-zfs/projects/lins2/Panos_Space/DATA/OUT/3d_neighbs_as_context/pilot_triplets/outlier_frac_30/one_far/data/model_names_of_triplets.txt'\n",
    "far_map_file = '/orions4-zfs/projects/lins2/Panos_Space/DATA/OUT/3d_neighbs_as_context/pilot_triplets/outlier_frac_30/both_far/data/model_names_of_triplets.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geo_embedding = pandas.read_pickle('/orions4-zfs/projects/lins2/Panos_Space/DATA/OUT/3d_neighbs_as_context/pilot_chair_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "close_feats, close_labels = process_data_of_single_condition(close_map_file, geo_embedding)\n",
    "far_feats, far_labels = process_data_of_single_condition(far_map_file, geo_embedding)\n",
    "split_feats, split_labels = process_data_of_single_condition(split_map_file, geo_embedding)\n",
    "\n",
    "all_cond_feats = np.vstack([close_feats, far_feats, split_feats])\n",
    "all_cond_labels = np.vstack([close_labels, far_labels, split_labels])\n",
    "\n",
    "# all_cond_feats = close_feats\n",
    "# all_cond_labels = close_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, _, test_data, _ = train_validate_test_split([all_cond_feats, all_cond_labels], train_perc=0.8, validate_perc=0, test_perc=0.2, shuffle=True)\n",
    "train_data = PointCloudDataSet(point_clouds=train_data[0], labels=train_data[1])\n",
    "test_data = PointCloudDataSet(point_clouds=test_data[0], labels=test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "geo_feat_size = 128 * n_classes\n",
    "learning_rate = 0.001\n",
    "train_epochs = 20\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_geometry = tf.placeholder(tf.float32, [None, geo_feat_size])\n",
    "target_label = tf.placeholder(tf.int32, [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = decoder_with_fc_only(input_geometry, layer_sizes=[100, n_classes])\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, target_label)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "prediction = tf.argmax(logits, axis=1)\n",
    "correct_pred = tf.equal(prediction, tf.argmax(target_label, axis=1))\n",
    "avg_accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "opt_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_config = tf.ConfigProto()\n",
    "gpu_config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=gpu_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0696971, 0.40000001]\n",
      "[0.95823818, 0.55000001]\n",
      "[0.76703447, 0.80000001]\n",
      "[0.54472804, 0.89999998]\n",
      "[0.52419209, 0.94999999]\n",
      "[0.26890132, 1.0]\n",
      "[0.23552378, 1.0]\n",
      "[0.1838578, 1.0]\n",
      "[0.21428573]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)    \n",
    "for epoch in range(20):\n",
    "    for _ in range(train_data.num_examples/batch_size):\n",
    "        batch_geo, batch_labels, _ = train_data.next_batch(batch_size)\n",
    "        feed_dict = {input_geometry: batch_geo, target_label: batch_labels}\n",
    "        sess.run([opt_step], feed_dict=feed_dict)\n",
    "    if epoch % 5 == 0:\n",
    "        print sess.run([cross_entropy, avg_accuracy], feed_dict=feed_dict)        \n",
    "\n",
    "# TEST DATA        \n",
    "batch_geo, batch_labels, _ = test_data.next_batch(test_data.num_examples)\n",
    "feed_dict = {input_geometry: batch_geo, target_label: batch_labels}\n",
    "print sess.run([avg_accuracy], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "I have deciphered the issue of the classification.\n",
    "There is no bias - or any problem that you have to worry about the data that I gave to Judy. (edited)\n",
    "In fact I got more intuition on the matter.\n",
    "Here is the two cents:  when we give to a NN only the latent geometric codes\n",
    "it can infer in the test data the “target” with ~33% when the training/test data come only from the  close condition.\n",
    "I.e., it does random guessing in the test data set.\n",
    "But when we train/test with the split/far condition respectively, it guesses 50%/60%  correctly in the test data.\n",
    "This is because it is clever enough to pick up the correlations implied by the way I construct the “far-split” conditions. I.e., in the far: both distractors are always “far” from the target, but we don’t constraint the between distance -> thus we give a clue to the network about who is the target. (edited)\n",
    "Similarly, we do for the “split”."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
