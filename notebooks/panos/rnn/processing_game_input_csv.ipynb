{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import matplotlib.pylab as plt\n",
    "from pandas import DataFrame\n",
    "from general_tools.in_out.basics import files_in_subdirs\n",
    "from general_tools.in_out.basics import pickle_data, unpickle_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "top_in_dir = '/orions4-zfs/projects/lins2/Panos_Space/DATA/Language/'\n",
    "dialogues_dir = osp.join(top_in_dir, 'pilot_chair_data/chatMessage')\n",
    "dialogue_files = [f for f in files_in_subdirs(dialogues_dir, '.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'don\\'t' in word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/orions4-zfs/projects/lins2/Panos_Space/DATA/Language/pilot_chair_data/chatMessage/2017-7-19-8h-31m-45s-1994-9aa59dbb-d55b-43bb-8d88-25974d5e073d.csv\n",
      "['far', '109']\n",
      "The one that looks the most boxy\n",
      "\n",
      "['far', '51']\n",
      "the one that looks like a fence\n",
      "\n",
      "['close', '95']\n",
      "the back of the chair is curved upwards\n",
      "\n",
      "['close', '57']\n",
      "one of the chairs legs looks awkwardly curved\n",
      "\n",
      "['far', '2']\n",
      "you can lay down on it\n",
      "\n",
      "['close', '86']\n",
      "there are two folding chairs that look alike. it is the one that has a simpler back.\n",
      "\n",
      "['far', '91']\n",
      "looks like a tube\n",
      "\n",
      "['split', '29']\n",
      "there are 2 with the fence back. the one where the posts are not at the ends.\n",
      "\n",
      "['close', '118']\n",
      "looks like a storage box\n",
      "\n",
      "['close', '111']\n",
      "low to the ground with a squiggle piece\n",
      "\n",
      "['far', '14']\n",
      "if you sat down and leaned back you would fall\n",
      "\n",
      "['close', '98']\n",
      "has some patterns on the back \n",
      "\n",
      "['split', '101']\n",
      "shortest chair\n",
      "\n",
      "['close', '135']\n",
      "two chairs look alike. pick the one with the squar back\n",
      "\n",
      "['far', '114']\n",
      "little fold up chair\n",
      "\n",
      "['far', '25']\n",
      "couch chair\n",
      "\n",
      "['far', '79']\n",
      "two rectangles put together chair\n",
      "\n",
      "['close', '31']\n",
      "two chairs look alike. pick the one with sqiggle on arm rest\n",
      "\n",
      "['split', '27']\n",
      "toilet chair with bigger tank\n",
      "\n",
      "['split', '16']\n",
      "two look alike. pick one with thicker arm rest\n",
      "\n",
      "['split', '107']\n",
      "the most skinny legs\n",
      "\n",
      "['close', '4']\n",
      "two chairs look alike. pick one with wider back and seating area\n",
      "\n",
      "['split', '106']\n",
      "fancy looking one\n",
      "\n",
      "['far', '58']\n",
      "almost looks like a back patio chair\n",
      "\n",
      "['split', '13']\n",
      "wierd chair with rectangle on back middle\n",
      "\n",
      "['far', '28']\n",
      "lean back fall over\n",
      "\n",
      "['close', '102']\n",
      "two chairs have wheels. pick one with skinny back\n",
      "\n",
      "['far', '6']\n",
      "chair that has a tail\n",
      "\n",
      "['split', '23']\n",
      "two chairs look alike. one with curves\n",
      "\n",
      "['split', '64']\n",
      "thick square \n",
      "\n",
      "['close', '39']\n",
      "chair has no legs\n",
      "\n",
      "['close', '130']\n",
      "looks like a person\n",
      "\n",
      "['close', '82']\n",
      "no back, thick legs\n",
      "\n",
      "['close', '67']\n",
      "two chairs look alike. the one where the grates don't go past back of chair\n",
      "\n",
      "['close', '84']\n",
      "looks lik ea fat chair\n",
      "\n",
      "['split', '113']\n",
      "looks like an S\n",
      "\n",
      "['far', '71']\n",
      "couch chair\n",
      "\n",
      "['close', '62']\n",
      "tallest\n",
      "\n",
      "['close', '8']\n",
      "shortest. thick legs\n",
      "\n",
      "['split', '123']\n",
      "regular back (not fence style). taller \n",
      "\n",
      "['split', '129']\n",
      "circle back\n",
      "\n",
      "['far', '74']\n",
      "you would fall back if you leaned back\n",
      "\n",
      "['split', '120']\n",
      "only two legs in front. none in back\n",
      "\n",
      "['split', '26']\n",
      "two look alike. pick one with wider seating and rounded seating\n",
      "\n",
      "['close', '68']\n",
      "looks like wheel chair\n",
      "\n",
      "['close', '89']\n",
      "boxy, thick arm rest, thick legs\n",
      "\n",
      "['split', '134']\n",
      "rounded top\n",
      "\n",
      "['far', '7']\n",
      "patio chair but wierd. Not too wierd though\n",
      "\n",
      "['split', '30']\n",
      "two look alike. simple rectangler back\n",
      "\n",
      "['split', '22']\n",
      "not very wide\n",
      "\n",
      "['far', '24']\n",
      "fat chunky chair\n",
      "\n",
      "['far', '138']\n",
      "1 sec\n",
      "\n",
      "['far', '138']\n",
      "shortest one\n",
      "\n",
      "['split', '69']\n",
      "two look the same, thicker arm rests\n",
      "\n",
      "['far', '122']\n",
      "desk chair you can write on\n",
      "\n",
      "['close', '66']\n",
      "wheel chair style\n",
      "\n",
      "['far', '116']\n",
      "tallest back\n",
      "\n",
      "['far', '9']\n",
      "tallest and filled in\n",
      "\n",
      "['split', '50']\n",
      "two look alike. wider seating\n",
      "\n",
      "['far', '18']\n",
      "hole on seat\n",
      "\n",
      "['close', '48']\n",
      "hole on back\n",
      "\n",
      "['split', '38']\n",
      "two look alike. rounder one, filled in\n",
      "\n",
      "['split', '59']\n",
      "no arm rests\n",
      "\n",
      "['close', '70']\n",
      "horizontal design on back\n",
      "\n",
      "['split', '117']\n",
      "two look alike. larger back\n",
      "\n",
      "['split', '121']\n",
      "3 holes on back\n",
      "\n",
      "['close', '128']\n",
      "life guard chair wth arms\n",
      "\n",
      "['far', '17']\n",
      "meaty chair no back\n",
      "\n",
      "['far', '35']\n",
      "desk chair\n",
      "\n",
      "['far', '133']\n",
      "curved rectangle back\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# game_file = dialogue_files[5]\n",
    "# in_csv = pd.read_csv(game_file, sep='\\t')\n",
    "# print game_file\n",
    "# for text, triplet in zip(in_csv['text'], in_csv['intendedName']):\n",
    "#     print triplet.split('_')[:2]\n",
    "#     print text + '\\n'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Language_Game_Data_Loader(): \n",
    "        \n",
    "    @staticmethod\n",
    "    def load_dialogues_of_game(game_file):\n",
    "        special_tokens = {'speaker': ' SpEaKeR: ', 'listener': ' lIsTeNeR: '}    \n",
    "            \n",
    "        # Hard-coded incides of interesting quantities.\n",
    "        triplet_id = 4\n",
    "        human_role = 5\n",
    "        sentence_pos = 6\n",
    "        trial_to_data = {}\n",
    "\n",
    "        with open(osp.join(top_in_dir, game_file), 'rb') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter='\\t')\n",
    "\n",
    "            for i, row in enumerate(reader):\n",
    "                if i == 0:                    \n",
    "                    continue\n",
    "                    \n",
    "                tr_id = row[triplet_id].split('_')\n",
    "                \n",
    "                if tr_id[0] not in ['far', 'split', 'close'] or not tr_id[1].isdigit():\n",
    "                    print 'Entry without proper triplet_id was found.'\n",
    "                    continue\n",
    "\n",
    "                tr_id = tr_id[0] + '_' + tr_id[1]    \n",
    "                role = special_tokens[row[human_role]]\n",
    "                sentence = row[sentence_pos]\n",
    "\n",
    "                if tr_id in trial_to_data:\n",
    "                    trial_to_data[tr_id]['dialogue'] += role + sentence\n",
    "                else:\n",
    "                    trial_to_data[tr_id] = dict()\n",
    "                    trial_to_data[tr_id]['dialogue'] = role + sentence\n",
    "            return trial_to_data\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_selections_of_game(game_file):\n",
    "        with open(osp.join(top_in_dir, game_file), 'rb') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                if i == 0:                    \n",
    "                    continue\n",
    "                print row[15]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _triplet_to_model_map(map_file):\n",
    "        res = dict()\n",
    "        with open (map_file, 'r') as fin:\n",
    "            for line in fin:\n",
    "                tokens = line.split()\n",
    "                res[tokens[0]] = (tokens[1], tokens[2], tokens[3])\n",
    "            return res\n",
    "\n",
    "    @staticmethod\n",
    "    def triplet_to_model_map(close_map_file, far_map_file, split_map_file):\n",
    "        triplet_to_model_map = {\n",
    "        'close': Language_Game_Data_Loader._triplet_to_model_map(close_map_file),\n",
    "        'far': Language_Game_Data_Loader._triplet_to_model_map(far_map_file),\n",
    "        'split': Language_Game_Data_Loader._triplet_to_model_map(split_map_file)}\n",
    "        return triplet_to_model_map\n",
    "\n",
    "        \n",
    "def load_glove_pretrained_model(glove_file):\n",
    "    print \"Loading glove model.\"\n",
    "    embedding = dict()\n",
    "    with open(glove_file, 'r') as f_in:\n",
    "        for line in f_in:\n",
    "            s_line = line.split()\n",
    "            word = s_line[0]\n",
    "            w_embedding = np.array([float(val) for val in s_line[1:]], dtype=np.float32)\n",
    "            embedding[word] = w_embedding\n",
    "    print \"Done.\", len(embedding), \" words loaded!\"\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "close_map_file = '/orions4-zfs/projects/lins2/Panos_Space/DATA/OUT/3d_neighbs_as_context/pilot_triplets/outlier_frac_30/closest_nn/data/model_names_of_triplets.txt'\n",
    "split_map_file = '/orions4-zfs/projects/lins2/Panos_Space/DATA/OUT/3d_neighbs_as_context/pilot_triplets/outlier_frac_30/one_far/data/model_names_of_triplets.txt'\n",
    "far_map_file = '/orions4-zfs/projects/lins2/Panos_Space/DATA/OUT/3d_neighbs_as_context/pilot_triplets/outlier_frac_30/both_far/data/model_names_of_triplets.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "triplet_to_model_names = Language_Game_Data_Loader.triplet_to_model_map(close_map_file, far_map_file, split_map_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry without proper triplet_id was found.\n",
      "Entry without proper triplet_id was found.\n",
      "Entry without proper triplet_id was found.\n",
      "Entry without proper triplet_id was found.\n",
      "Entry without proper triplet_id was found.\n",
      "Entry without proper triplet_id was found.\n"
     ]
    }
   ],
   "source": [
    "language_data = []\n",
    "for f in dialogue_files:\n",
    "    language_data.append(Language_Game_Data_Loader.load_dialogues_of_game(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading glove model.\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "geo_embedding = pandas.read_pickle('/orions4-zfs/projects/lins2/Panos_Space/DATA/OUT/3d_neighbs_as_context/pilot_chair_embeddings.pkl')\n",
    "word_emb_file = '/orions4-zfs/projects/lins2/Panos_Space/DATA/Language/glove.6B/glove.6B.100d.txt'\n",
    "word_dict = load_glove_pretrained_model(word_emb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def embedding_dictionary_to_matrix(in_dict):\n",
    "    return np.array(in_dict.values())\n",
    "\n",
    "def replace_all(text, dic):\n",
    "    for i, j in dic.iteritems():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "replace_dic = {'!': ' ', ',': ' ', '?': ' ', '.': ' ', ';': ' ', '(': ' ', ')': ' ', '/': ' ',\n",
    "               '&': 'and', '\"': ' ', '-': ' ', '=': ' ', '*': ' ',                \n",
    "               '\\'t': ' not', '\\'s': ' of'               \n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Deal with \"speaker:\", \"listener:\"\n",
    "# misspelled words\n",
    "# dentist's chair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Use tf.nn.embedding_lookup instead and map words to integers\n",
    "words = set()\n",
    "words_missed = set()\n",
    "words_embedded = []\n",
    "all_data = dict()\n",
    "\n",
    "for i, game in enumerate(language_data):\n",
    "    for triplet_id, dialogues in game.iteritems():\n",
    "        triplet_condition, triplet_num = triplet_id.split('_')\n",
    "        a, b, c = triplet_to_model_names[triplet_condition][triplet_num]\n",
    "        triplet_geo_emb = geo_embedding.loc[[a, b, c]].as_matrix().T\n",
    "        index = np.arange(3)\n",
    "        np.random.shuffle(index)\n",
    "        triplet_geo_emb = triplet_geo_emb[:, index]\n",
    "        label = np.where(index==0)[0][0]\n",
    "        \n",
    "        triplet_word_seq = []        \n",
    "        for w in replace_all(dialogues['dialogue'], replace_dic).split():\n",
    "            wl = w.lower()\n",
    "            if wl in word_dict:\n",
    "                triplet_word_seq.append(word_dict[wl])\n",
    "                words.add(wl)                \n",
    "            else: # TODO deal with uknown words\n",
    "#                 'unk' in word_dict\n",
    "                words_missed.add(wl)\n",
    "        \n",
    "        if len(triplet_word_seq) >= 1:\n",
    "            if triplet_id not in all_data:\n",
    "                all_data[triplet_id] = {}\n",
    "                \n",
    "            all_data[triplet_id][i] = {}\n",
    "            all_data[triplet_id][i]['label'] = label\n",
    "            all_data[triplet_id][i]['word_feats'] =  np.array(triplet_word_seq)\n",
    "            all_data[triplet_id][i]['geo_feats'] = triplet_geo_emb\n",
    "            all_data[triplet_id][i]['condition'] = triplet_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "unique_targets = set()\n",
    "for key in all_data:\n",
    "    unique_targets.add(key.split('_')[1])\n",
    "\n",
    "unique_targets = np.array(list(unique_targets), dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "conditions = ['far', 'split', 'close']\n",
    "samples_per_target = []\n",
    "for target in unique_targets:\n",
    "    t_samples = 0\n",
    "    for cond in conditions:        \n",
    "        key = cond + '_' + target\n",
    "        if key in all_data:\n",
    "            t_samples += len(all_data[key])\n",
    "    samples_per_target.append(t_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['138' '135' '130' '25' '11' '5' '95' '118' '22' '113']\n"
     ]
    }
   ],
   "source": [
    "# exclude_for_test_data = np.argsort(samples_per_target)[:10]   % those with fewest samples\n",
    "\n",
    "exclude_for_test_data = np.random.choice(len(unique_targets), 10, replace=False)\n",
    "exclude_for_test_data = unique_targets[exclude_for_test_data]\n",
    "print exclude_for_test_data\n",
    "\n",
    "\n",
    "test_data = {}\n",
    "train_data = {}\n",
    "\n",
    "for triplet_id in all_data:\n",
    "    target = triplet_id.split('_')[1]\n",
    "    \n",
    "    if target in exclude_for_test_data:\n",
    "        test_data[triplet_id] = all_data[triplet_id]\n",
    "    else:\n",
    "        train_data[triplet_id] = all_data[triplet_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sequences_to_numpy_arrays(in_seq_data, max_seq_len=30, word_feat_dim=100, geo_feat_dim=128):\n",
    "    n_triplets = 0\n",
    "    for triplet_id in in_seq_data:\n",
    "        n_triplets += len(in_seq_data[triplet_id])\n",
    "    \n",
    "    word_data = np.zeros(shape=(n_triplets, max_seq_len, word_feat_dim))\n",
    "    all_seq_len = []\n",
    "\n",
    "    i = 0\n",
    "    for triplet_id in in_seq_data:\n",
    "        for key in in_seq_data[triplet_id]: # iterate over different users/players\n",
    "            datum = in_seq_data[triplet_id][key]['word_feats']\n",
    "            n_steps = datum.shape[0]\n",
    "            all_seq_len.append(n_steps)        \n",
    "            pad_many = max_seq_len-n_steps\n",
    "            if pad_many >= 0:\n",
    "                word_data[i] = np.pad(datum, ((0, pad_many), (0,0)) , 'constant')\n",
    "            elif pad_many < 0:\n",
    "                word_data[i] = datum[:max_seq_len,:]\n",
    "            i += 1\n",
    "    \n",
    "    geo_data = np.zeros(shape=(n_triplets, 3*geo_feat_dim))\n",
    "    label_data = np.zeros((n_triplets, 3), dtype=np.int32)\n",
    "    condition_data = np.empty(shape=(n_triplets,), dtype=object)\n",
    "    \n",
    "    i = 0\n",
    "    for triplet_id in in_seq_data:\n",
    "        for key in in_seq_data[triplet_id]:\n",
    "            datum = in_seq_data[triplet_id][key]['geo_feats']       \n",
    "            geo_data[i] = np.hstack(datum)\n",
    "            \n",
    "            label = in_seq_data[triplet_id][key]['label']\n",
    "            label_data[i, label] = 1\n",
    "            \n",
    "            condition_data[i] = in_seq_data[triplet_id][key]['condition'] # TEMP\n",
    "            i += 1\n",
    "    return word_data, geo_data, label_data, condition_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class GeoWordsDataSet(object):\n",
    "\n",
    "    def __init__(self, words, geometries, labels, conditions, use_copies=True, init_shuffle=False):\n",
    "        self.num_examples = words.shape[0]\n",
    "        assert(self.num_examples == labels.shape[0])\n",
    "        assert(self.num_examples == geometries.shape[0])\n",
    "        \n",
    "        assert( use_copies )\n",
    "        assert(init_shuffle==False)\n",
    "        \n",
    "        self.words = words.copy()        \n",
    "        self.geometries = geometries.copy()\n",
    "        self.labels = labels.copy()\n",
    "        self.conditions = conditions.copy()\n",
    "\n",
    "        self.epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "        if init_shuffle:\n",
    "            self.shuffle_data()\n",
    "            \n",
    "    def shuffle_data(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        perm = np.arange(self.num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        self.words = self.words[perm]\n",
    "        self.geometries = self.geometries[perm]\n",
    "        self.labels = self.labels[perm]\n",
    "        return self\n",
    "\n",
    "    def next_batch(self, batch_size, seed=None):\n",
    "        '''Return the next batch_size examples from this data set.\n",
    "        '''\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += batch_size\n",
    "        if self._index_in_epoch > self.num_examples:\n",
    "            self.epochs_completed += 1  # Finished epoch.\n",
    "            self.shuffle_data(seed)\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size\n",
    "        end = self._index_in_epoch\n",
    "\n",
    "        return self.words[start:end], self.geometries[start:end], self.labels[start:end], self.conditions[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_data, geo_data, label_data, cond_data = sequences_to_numpy_arrays(train_data)\n",
    "gw_data = GeoWordsDataSet(words=word_data, geometries=geo_data, labels=label_data, conditions=cond_data)\n",
    "pickle_data('first_pilot_training_data.pkl', gw_data)\n",
    "\n",
    "word_data, geo_data, label_data, cond_data = sequences_to_numpy_arrays(test_data)\n",
    "gw_data = GeoWordsDataSet(words=word_data, geometries=geo_data, labels=label_data, conditions=cond_data)\n",
    "pickle_data('first_pilot_test_data.pkl', gw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None, None, None, None, None, None, None, None, None, None], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# map numbers to language\n",
    "# typos\n",
    "\n",
    "\n",
    "# sqare -> square \n",
    "# sqiggle -> squiggly\n",
    "# shorest -> shortest\n",
    "# pilow -> pillow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
