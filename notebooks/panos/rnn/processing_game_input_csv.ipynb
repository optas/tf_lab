{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "from pandas import DataFrame\n",
    "from general_tools.in_out.basics import files_in_subdirs\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "top_in_dir = '/orions4-zfs/projects/lins2/Panos_Space/DATA/Language/'\n",
    "dialogues_dir = osp.join(top_in_dir, 'pilot_chair_data/chatMessage')\n",
    "dialogue_files = [f for f in files_in_subdirs(dialogues_dir, '.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Language_Game_Data_Loader(): \n",
    "        \n",
    "    @staticmethod\n",
    "    def load_dialogues_of_game(game_file):\n",
    "        special_tokens = {'speaker': ' SpEaKeR: ', 'listener': ' lIsTeNeR: '}    \n",
    "            \n",
    "        # Hard-coded incides of interesting quantities.\n",
    "        triplet_id = 4\n",
    "        human_role = 5\n",
    "        sentence_pos = 6\n",
    "        trial_to_data = {}\n",
    "\n",
    "        with open(osp.join(top_in_dir, game_file), 'rb') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter='\\t')\n",
    "\n",
    "            for i, row in enumerate(reader):\n",
    "                if i == 0:                    \n",
    "                    continue\n",
    "                    \n",
    "                tr_id = row[triplet_id].split('_')\n",
    "                \n",
    "                if tr_id[0] not in ['far', 'split', 'close'] or not tr_id[1].isdigit():\n",
    "                    print 'Entry without proper triplet_id was found.'\n",
    "                    continue\n",
    "\n",
    "                tr_id = tr_id[0] + '_' + tr_id[1]    \n",
    "                role = special_tokens[row[human_role]]\n",
    "                sentence = row[sentence_pos]\n",
    "\n",
    "                if tr_id in trial_to_data:\n",
    "                    trial_to_data[tr_id]['dialogue'] += role + sentence\n",
    "                else:\n",
    "                    trial_to_data[tr_id] = dict()\n",
    "                    trial_to_data[tr_id]['dialogue'] = role + sentence\n",
    "            return trial_to_data\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_selections_of_game(game_file):\n",
    "        with open(osp.join(top_in_dir, game_file), 'rb') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                if i == 0:                    \n",
    "                    continue\n",
    "                print row[15]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _triplet_to_model_map(map_file):\n",
    "        res = dict()\n",
    "        with open (map_file, 'r') as fin:\n",
    "            for line in fin:\n",
    "                tokens = line.split()\n",
    "                res[tokens[0]] = (tokens[1], tokens[2], tokens[3])\n",
    "            return res\n",
    "\n",
    "    @staticmethod    \n",
    "    def triplet_to_model_map(close_map_file, far_map_file, split_map_file):\n",
    "        triplet_to_model_map = {\n",
    "        'close': Language_Game_Data_Loader._triplet_to_model_map(close_map_file),\n",
    "        'far': Language_Game_Data_Loader._triplet_to_model_map(far_map_file),\n",
    "        'split': Language_Game_Data_Loader._triplet_to_model_map(split_map_file)}\n",
    "        return triplet_to_model_map\n",
    "\n",
    "        \n",
    "def load_glove_pretrained_model(glove_file):\n",
    "    print \"Loading glove model.\"\n",
    "    embedding = dict()\n",
    "    with open(glove_file, 'r') as f_in:\n",
    "        for line in f_in:\n",
    "            s_line = line.split()\n",
    "            word = s_line[0]\n",
    "            w_embedding = np.array([float(val) for val in s_line[1:]], dtype=np.float32)\n",
    "            embedding[word] = w_embedding\n",
    "    print \"Done.\", len(embedding), \" words loaded!\"\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "close_map_file = '/orions4-zfs/projects/lins2/Panos_Space/DATA/OUT/3d_neighbs_as_context/pilot_triplets/outlier_frac_30/closest_nn/data/model_names_of_triplets.txt'\n",
    "split_map_file = '/orions4-zfs/projects/lins2/Panos_Space/DATA/OUT/3d_neighbs_as_context/pilot_triplets/outlier_frac_30/one_far/data/model_names_of_triplets.txt'\n",
    "far_map_file = '/orions4-zfs/projects/lins2/Panos_Space/DATA/OUT/3d_neighbs_as_context/pilot_triplets/outlier_frac_30/both_far/data/model_names_of_triplets.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "triplet_to_model_names = Language_Game_Data_Loader.triplet_to_model_map(close_map_file, far_map_file, split_map_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry without proper triplet_id was found.\n",
      "Entry without proper triplet_id was found.\n",
      "Entry without proper triplet_id was found.\n",
      "Entry without proper triplet_id was found.\n",
      "Entry without proper triplet_id was found.\n",
      "Entry without proper triplet_id was found.\n"
     ]
    }
   ],
   "source": [
    "language_data = []\n",
    "for f in dialogue_files:\n",
    "    language_data.append(Language_Game_Data_Loader.load_dialogues_of_game(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading glove model.\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "geo_embedding = pandas.read_pickle('/orions4-zfs/projects/lins2/Panos_Space/DATA/OUT/3d_neighbs_as_context/pilot_chair_embeddings.pkl')\n",
    "word_emb_file = '/orions4-zfs/projects/lins2/Panos_Space/DATA/Language/glove.6B/glove.6B.100d.txt'\n",
    "word_dict = load_glove_pretrained_model(word_emb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def embedding_dictionary_to_matrix(in_dict):\n",
    "    return np.array(in_dict.values())\n",
    "\n",
    "def replace_all(text, dic):\n",
    "    for i, j in dic.iteritems():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "replace_dic = {'!': ' ', ',': ' ', '?': ' ', '.': ' ', ';': ' ', '(': ' ', ')': ' ', '/': ' ',\n",
    "               '&': 'and', '\"': ' ', '-': ' ', '=': ' '\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Deal with \"speaker:\", \"listener:\"\n",
    "# misspelled words\n",
    "# dentist's chair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Use tf.nn.embedding_lookup instead and map words to integers\n",
    "\n",
    "words = set()\n",
    "words_embedded = []\n",
    "train_data = dict()\n",
    "\n",
    "for i, game in enumerate(language_data):\n",
    "    for triplet_id, dialogues in game.iteritems():\n",
    "        triplet_condition, triplet_num = triplet_id.split('_')\n",
    "        a, b, c = triplet_to_model_names[triplet_condition][triplet_num]\n",
    "        triplet_geo_emb = geo_embedding.loc[[a, b, c]].as_matrix().T\n",
    "        index = np.arange(3)\n",
    "        np.random.shuffle(index)\n",
    "        triplet_geo_emb = triplet_geo_emb[:, index]\n",
    "        label = np.where(index==0)[0][0]\n",
    "        \n",
    "        triplet_word_seq = []        \n",
    "        for w in replace_all(dialogues['dialogue'], replace_dic).split():\n",
    "            wl = w.lower()\n",
    "            if wl in word_dict:\n",
    "                triplet_word_seq.append(word_dict[wl])\n",
    "                words.add(wl)                \n",
    "            else: #TODO deal with uknown words\n",
    "#                 'unk' in word_dict\n",
    "                pass \n",
    "        \n",
    "        if len(triplet_word_seq) >= 1:\n",
    "            train_data[(i, triplet_id)] = {'label': label}\n",
    "            train_data[(i, triplet_id)]['word_feats'] =  np.array(triplet_word_seq)\n",
    "            train_data[(i, triplet_id)]['geo_feats'] = triplet_geo_emb.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1',\n",
       " '10',\n",
       " '101',\n",
       " '102',\n",
       " '106',\n",
       " '107',\n",
       " '109',\n",
       " '11',\n",
       " '111',\n",
       " '113',\n",
       " '114',\n",
       " '116',\n",
       " '117',\n",
       " '118',\n",
       " '12',\n",
       " '120',\n",
       " '121',\n",
       " '122',\n",
       " '123',\n",
       " '128',\n",
       " '129',\n",
       " '13',\n",
       " '130',\n",
       " '133',\n",
       " '134',\n",
       " '135',\n",
       " '138',\n",
       " '14',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '2',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '30',\n",
       " '31',\n",
       " '35',\n",
       " '38',\n",
       " '39',\n",
       " '4',\n",
       " '48',\n",
       " '5',\n",
       " '50',\n",
       " '51',\n",
       " '57',\n",
       " '58',\n",
       " '59',\n",
       " '6',\n",
       " '60',\n",
       " '62',\n",
       " '64',\n",
       " '66',\n",
       " '67',\n",
       " '68',\n",
       " '69',\n",
       " '7',\n",
       " '70',\n",
       " '71',\n",
       " '74',\n",
       " '79',\n",
       " '8',\n",
       " '82',\n",
       " '84',\n",
       " '86',\n",
       " '89',\n",
       " '9',\n",
       " '91',\n",
       " '95',\n",
       " '98'}"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(key[1].split('_')[1] for key in train_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_triplets = len(train_data)\n",
    "max_seq_len = 20\n",
    "example = train_data[train_data.keys()[0]]\n",
    "word_data = np.zeros(shape=((n_triplets, max_seq_len,example['word_feats'].shape[1])))\n",
    "\n",
    "all_seq_len = []\n",
    "\n",
    "for i, key in enumerate(train_data):\n",
    "    datum = train_data[key]['word_feats']       \n",
    "    n_steps = datum.shape[0]\n",
    "    all_seq_len.append(n_steps)        \n",
    "    pad_many = max_seq_len-n_steps\n",
    "    if pad_many > 0:\n",
    "        word_data[i] = np.pad(datum, ((0, pad_many), (0,0)) , 'constant')\n",
    "    elif pad_many < 0:\n",
    "        word_data[i] = train_data[key]['word_feats'][:max_seq_len,:]\n",
    "    \n",
    "\n",
    "geo_data = np.zeros(shape=(n_triplets, np.prod(example['geo_feats'].shape)) )\n",
    "label_data = np.zeros((n_triplets, 3), dtype=np.int32)\n",
    "for i, key in enumerate(train_data):\n",
    "    datum = train_data[key]['geo_feats']       \n",
    "    geo_data[i] = np.hstack(datum)\n",
    "    label = train_data[key]['label']\n",
    "    label_data[i][label] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GeoWordsDataSet(object):\n",
    "\n",
    "    def __init__(self, words, geometries, labels, use_copies=True, init_shuffle=False):\n",
    "        self.num_examples = words.shape[0]\n",
    "        assert(self.num_examples == labels.shape[0])\n",
    "        assert(self.num_examples == geometries.shape[0])\n",
    "        \n",
    "        assert( use_copies )\n",
    "        assert(init_shuffle==False)\n",
    "        \n",
    "        self.words = words.copy()        \n",
    "        self.geometries = geometries.copy()\n",
    "        self.labels = labels.copy()\n",
    "\n",
    "        self.epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "        if init_shuffle:\n",
    "            self.shuffle_data()\n",
    "            \n",
    "    def shuffle_data(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        perm = np.arange(self.num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        self.words = self.words[perm]\n",
    "        self.geometries = self.geometries[perm]\n",
    "        self.labels = self.labels[perm]\n",
    "        return self\n",
    "\n",
    "    def next_batch(self, batch_size, seed=None):\n",
    "        '''Return the next batch_size examples from this data set.\n",
    "        '''\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += batch_size\n",
    "        if self._index_in_epoch > self.num_examples:\n",
    "            self.epochs_completed += 1  # Finished epoch.\n",
    "            self.shuffle_data(seed)\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size\n",
    "        end = self._index_in_epoch\n",
    "\n",
    "        return self.words[start:end], self.geometries[start:end], self.labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gw_data = GeoWordsDataSet(words=word_data, geometries=geo_data, labels=label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from general_tools.in_out.basics import pickle_data, unpickle_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_data('first_pilot_data.pkl', gw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lala = unpickle_data('first_pilot_data.pkl').next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1]], dtype=int32)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lala.next_batch(1)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# map numbers to language\n",
    "# typos\n",
    "\n",
    "\n",
    "# sqare -> square \n",
    "# sqiggle -> squiggly\n",
    "# shorest -> shortest\n",
    "# pilow -> pillow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
