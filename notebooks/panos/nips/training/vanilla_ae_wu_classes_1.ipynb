{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picking GPU 0\n"
     ]
    }
   ],
   "source": [
    "from general_tools.notebook.gpu_utils import setup_one_gpu\n",
    "GPU = 0\n",
    "setup_one_gpu(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orions4-zfs/projects/lins2/Panos_Space/Git_Repos/geo_tool/solids/mesh.py:26: UserWarning: Mayavi library was not found. Some graphics utilities will be disabled.\n",
      "  warnings.warn('Mayavi library was not found. Some graphics utilities will be disabled.')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from general_tools.notebook.tf import reset_tf_graph\n",
    "\n",
    "import tf_lab.point_clouds.in_out as pio\n",
    "\n",
    "from tf_lab.point_clouds.in_out import PointCloudDataSet, write_model_ids_of_datasets\n",
    "from tf_lab.point_clouds.point_net_ae import PointNetAutoEncoder\n",
    "from tf_lab.point_clouds.autoencoder import Configuration as Conf\n",
    "import tf_lab.point_clouds.encoders_decoders as enc_dec\n",
    "\n",
    "\n",
    "from tf_lab.autopredictors.scripts.helper import shape_net_category_to_synth_id\n",
    "\n",
    "\n",
    "from tf_lab.autopredictors.plotting import plot_original_pclouds_vs_reconstructed, \\\n",
    "                                           plot_train_val_test_curves, plot_reconstructions_at_epoch\n",
    "\n",
    "\n",
    "from tf_lab.autopredictors.evaluate import eval_model, read_saved_epochs\n",
    "                                                  \n",
    "\n",
    "from general_tools.in_out.basics import create_dir, delete_files_in_directory, files_in_subdirs\n",
    "from general_tools.simpletons import select_first_last_and_k\n",
    "from geo_tool import Point_Cloud\n",
    "\n",
    "from tf_lab.nips.shape_net import pc_loader as sn_pc_loader\n",
    "from tf_lab.nips.helper import load_shape_net_models_used_by_wu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_pc_samples = 2048\n",
    "do_training = True\n",
    "first_time_running = True\n",
    "load_model = False\n",
    "z_rotate = True\n",
    "seed = 42\n",
    "max_training_epochs = 2000\n",
    "loss = 'chamfer'\n",
    "is_convolutional = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if is_convolutional:\n",
    "    arch_tag = 'convolutional_arch'\n",
    "    \n",
    "    encoder_args = {'n_filters': [128, 128, 256, 512],\n",
    "                    'filter_sizes': [40, 20, 10, 10],\n",
    "                    'strides': [1, 2, 2, 1]\n",
    "                   }\n",
    "        \n",
    "else:\n",
    "    arch_tag = 'mlp_arch'\n",
    "    encoder_args = {'n_filters': [64, 128, 256, 512],\n",
    "                    'filter_sizes' :[1, 1, 1, 1],\n",
    "                    'strides': [1, 1, 1, 1]\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment_name = 'wu_classes_rotated_' + arch_tag + '_' + str(n_pc_samples) +  'pts_' + loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/orions4-zfs/projects/lins2/Panos_Space/DATA/OUT/models/nips/vanilla_ae/wu_classes_rotated_convolutional_arch_2048pts_chamfer'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_data_dir = '/orions4-zfs/projects/lins2/Panos_Space/DATA/'\n",
    "full_pclouds_path = osp.join(top_data_dir, 'Point_Clouds/Shape_Net/Core/from_manifold_meshes/centered/', str(n_pc_samples))\n",
    "train_dir = osp.join(top_data_dir, 'OUT/models/nips/vanilla_ae/')\n",
    "train_dir = osp.join(train_dir, experiment_name)\n",
    "create_dir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane 02691156\n",
      "4045 files containing complete point clouds were found.\n",
      "car 02958343\n",
      "7497 files containing complete point clouds were found.\n",
      "chair 03001627\n",
      "6778 files containing complete point clouds were found.\n",
      "sofa 04256520\n",
      "3173 files containing complete point clouds were found.\n",
      "rifle 04090263\n",
      "2372 files containing complete point clouds were found.\n",
      "boat 02858304\n",
      "1137 files containing complete point clouds were found.\n",
      "table 04379243\n",
      "8509 files containing complete point clouds were found.\n"
     ]
    }
   ],
   "source": [
    "full_pclouds, full_model_names, full_syn_ids = load_shape_net_models_used_by_wu(n_pc_samples, full_pclouds_path)\n",
    "model_unames = full_model_names + '.' + full_syn_ids\n",
    "train_data = PointCloudDataSet(full_pclouds, labels=model_unames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if load_model:\n",
    "    conf = Conf.load(osp.join(train_dir, 'configuration'))\n",
    "    print conf\n",
    "    \n",
    "else:\n",
    "    n_input = [n_pc_samples, 3]\n",
    "    \n",
    "    decoder_args = {'layer_sizes': [1024, 2048, np.prod(n_input)] }\n",
    "    \n",
    "    conf = Conf(\n",
    "                n_input = n_input,\n",
    "                training_epochs = max_training_epochs,\n",
    "                batch_size = 50,\n",
    "                loss = loss,\n",
    "                denoising = False,\n",
    "                train_dir = train_dir,\n",
    "                loss_display_step = 1,\n",
    "                saver_step = 10,\n",
    "                learning_rate = 0.0005,\n",
    "                z_rotate = z_rotate,\n",
    "                encoder = enc_dec.encoder_with_convs_and_symmetry,\n",
    "                encoder_args = encoder_args,\n",
    "                decoder = enc_dec.decoder_with_fc_only,\n",
    "                decoder_args = decoder_args,        \n",
    "               )\n",
    "\n",
    "    conf.experiment_name = experiment_name\n",
    "    conf.save(osp.join(conf.train_dir, 'configuration'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reset_tf_graph()\n",
    "ae = PointNetAutoEncoder(experiment_name, conf)\n",
    "\n",
    "if load_model:\n",
    "    saved_epochs = read_saved_epochs(conf.train_dir)\n",
    "    last_epoch = saved_epochs[-1]\n",
    "    ae.restore_model(conf.train_dir, last_epoch, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', '0001', 'training time (minutes)=', '0.9614', 'loss=', '0.004922643')\n",
      "INFO:tensorflow:/orions4-zfs/projects/lins2/Panos_Space/DATA/OUT/models/nips/vanilla_ae/wu_classes_rotated_convolutional_arch_2048pts_chamfer/models.ckpt-1 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "('Epoch:', '0002', 'training time (minutes)=', '0.9573', 'loss=', '0.002178582')\n",
      "('Epoch:', '0003', 'training time (minutes)=', '0.9761', 'loss=', '0.001591790')\n",
      "('Epoch:', '0004', 'training time (minutes)=', '0.9763', 'loss=', '0.001450273')\n",
      "('Epoch:', '0005', 'training time (minutes)=', '0.9820', 'loss=', '0.001324086')\n",
      "('Epoch:', '0006', 'training time (minutes)=', '1.0421', 'loss=', '0.001285170')\n",
      "('Epoch:', '0007', 'training time (minutes)=', '1.4859', 'loss=', '0.001170775')\n",
      "('Epoch:', '0008', 'training time (minutes)=', '1.4086', 'loss=', '0.001192071')\n"
     ]
    }
   ],
   "source": [
    "if do_training:\n",
    "    training_stats = []\n",
    "    training_stats.append(ae.train(train_data, conf))    \n",
    "    with open(osp.join(conf.train_dir, 'train_stats.txt'), 'a') as fout:\n",
    "        np.savetxt(fout, np.array(training_stats)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
