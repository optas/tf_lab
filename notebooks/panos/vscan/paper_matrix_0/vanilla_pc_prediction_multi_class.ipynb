{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import hmean\n",
    "    \n",
    "from tf_lab.fundamentals.utils import set_visible_GPUs, reset_tf_graph\n",
    "\n",
    "import tf_lab.point_clouds.in_out as pio\n",
    "from tf_lab.point_clouds.in_out import PointCloudDataSet, write_model_ids_of_datasets\n",
    "from tf_lab.point_clouds.point_net_ae import PointNetAutoEncoder\n",
    "from tf_lab.point_clouds.autoencoder import Configuration as Conf\n",
    "import tf_lab.point_clouds.encoders_decoders as enc_dec\n",
    "\n",
    "\n",
    "import tf_lab.autopredictors.scripts.virt_scan_data as vscan\n",
    "\n",
    "from tf_lab.autopredictors.scripts.helper import shape_net_category_to_synth_id, points_extension, \\\n",
    "                                                 shape_net_core_synth_id_to_category\n",
    "\n",
    "\n",
    "from tf_lab.autopredictors.plotting import plot_original_pclouds_vs_reconstructed, \\\n",
    "                                           plot_train_val_test_curves, plot_reconstructions_at_epoch\n",
    "        \n",
    "from tf_lab.autopredictors.evaluate import eval_model, read_saved_epochs, accuracy_of_completion, \\\n",
    "                                           coverage_of_completion, save_reconstructions, \\\n",
    "                                           save_pc_prediction_stats, save_stats_of_multi_class_experiments\n",
    "                                                  \n",
    "\n",
    "from general_tools.in_out.basics import create_dir, delete_files_in_directory, files_in_subdirs\n",
    "from general_tools.simpletons import select_first_last_and_k\n",
    "from geo_tool import Point_Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GPU = 2\n",
    "exp_counter = '9'\n",
    "loss = 'emd'\n",
    "do_training = True\n",
    "load_model_conf = False\n",
    "do_evaluation = True\n",
    "\n",
    "incomplete_n_samples = 2048\n",
    "complete_n_samples = 4096\n",
    "val_percent = .10\n",
    "dropout_keep_prob = .8\n",
    "seed = 42\n",
    "\n",
    "experiment_name = exp_counter + '_all_classes_' + str(incomplete_n_samples) + '_' \\\n",
    "                  + str(complete_n_samples) + 'pts_' + loss\n",
    "\n",
    "top_data_dir = '/orions4-zfs/projects/lins2/Panos_Space/DATA/'\n",
    "\n",
    "n_input = [incomplete_n_samples, 3]\n",
    "n_output = [complete_n_samples, 3] \n",
    "\n",
    "train_dir = osp.join(top_data_dir, 'OUT/models/incomplete_pclouds/paper_vanilla_vscan')\n",
    "train_dir = osp.join(train_dir, experiment_name)\n",
    "create_dir(train_dir)\n",
    "\n",
    "if loss == 'emd':\n",
    "    max_training_epochs = 100\n",
    "elif loss == 'chamfer':\n",
    "    max_training_epochs = 300\n",
    "    \n",
    "max_evaluation_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4045 files containing complete point clouds were found.\n",
      "19800 incomplete point clouds were loaded.\n",
      "4470 incomplete point clouds were loaded.\n",
      "1572 files containing complete point clouds were found.\n",
      "7800 incomplete point clouds were loaded.\n",
      "1632 incomplete point clouds were loaded.\n",
      "7497 files containing complete point clouds were found.\n",
      "29640 incomplete point clouds were loaded.\n",
      "5922 incomplete point clouds were loaded.\n",
      "6778 files containing complete point clouds were found.\n",
      "30000 incomplete point clouds were loaded.\n",
      "6000 incomplete point clouds were loaded.\n",
      "2318 files containing complete point clouds were found.\n",
      "11100 incomplete point clouds were loaded.\n",
      "2808 incomplete point clouds were loaded.\n",
      "3173 files containing complete point clouds were found.\n",
      "15600 incomplete point clouds were loaded.\n",
      "3438 incomplete point clouds were loaded.\n",
      "8509 files containing complete point clouds were found.\n",
      "30000 incomplete point clouds were loaded.\n",
      "6000 incomplete point clouds were loaded.\n",
      "1939 files containing complete point clouds were found.\n",
      "9600 incomplete point clouds were loaded.\n",
      "2034 incomplete point clouds were loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_lab.point_clouds.in_out.PointCloudDataSet at 0x7f5ac305fed0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data of All Classes.\n",
    "class_to_syn_id = shape_net_category_to_synth_id()\n",
    "all_classes = vscan.all_classes\n",
    "n_threads = 50\n",
    "\n",
    "first = class_to_syn_id[all_classes[0]]\n",
    "train_data, val_data, test_data = vscan.load_train_val_test_vscan_paper(first, n_threads,\\\n",
    "                                                                        complete_n_samples=complete_n_samples,\\\n",
    "                                                                        incomplete_n_samples=incomplete_n_samples,\n",
    "                                                                        val_percent=val_percent)\n",
    "for model_class in vscan.all_classes[1:]:\n",
    "    class_syn_id = class_to_syn_id[model_class]\n",
    "    curr_train, curr_val, curr_test = vscan.load_train_val_test_vscan_paper(class_syn_id, n_threads,\\\n",
    "                                                              complete_n_samples=complete_n_samples,\\\n",
    "                                                              incomplete_n_samples=incomplete_n_samples,\\\n",
    "                                                              val_percent=val_percent)\n",
    "    train_data.merge(curr_train)\n",
    "    test_data.merge(curr_test)\n",
    "    val_data.merge(curr_val)\n",
    "\n",
    "train_data.shuffle_data();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153540\n",
      "32304\n"
     ]
    }
   ],
   "source": [
    "print train_data.num_examples + val_data.num_examples\n",
    "print test_data.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Verification we didn't mix train-test-val data.\n",
    "# tr = train_data.full_epoch_data(shuffle=False)\n",
    "# va = val_data.full_epoch_data(shuffle=False)\n",
    "# te = test_data.full_epoch_data(shuffle=False)\n",
    "\n",
    "# train_set = set([i[:-6] for i in tr[1]])\n",
    "# val_set = set([i[:-6] for i in va[1]])\n",
    "# test_set = set([i[:-6] for i in te[1]])\n",
    "\n",
    "# c1 = len(test_set.intersection(train_set)) == 0\n",
    "# c2 = len(test_set.intersection(val_set)) == 0\n",
    "# c3 = len(train_set.intersection(val_set)) == 0\n",
    "\n",
    "# assert(c1 and c2 and c3)\n",
    "\n",
    "# pp = test_data.next_batch(1)\n",
    "# pinc = pp[2].reshape(incomplete_n_samples, 3)\n",
    "# pcom = pp[0].reshape(complete_n_samples, 3)\n",
    "        \n",
    "# score1 = accuracy_of_completion(pinc, pcom, 0.02, ret_dists=False)\n",
    "# print score1\n",
    "# score2, c2 = coverage_of_completion(pcom, pinc, 0.02, ret_dists=True)\n",
    "\n",
    "# Point_Cloud(points=pinc).plot();\n",
    "# Point_Cloud(points=pcom).plot(c=c2);\n",
    "# print pp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(seed)\n",
    "\n",
    "if load_model_conf:\n",
    "    conf = Conf.load(osp.join(train_dir, 'configuration'))\n",
    "    print conf\n",
    "else:\n",
    "    decoder_args = {'layer_sizes': [1024, np.prod(n_output)],\n",
    "                    'non_linearity': tf.nn.relu\n",
    "                   }\n",
    "\n",
    "    encoder_args = {'dropout_prob': dropout_keep_prob}\n",
    "    \n",
    "    conf = Conf(\n",
    "                n_input = n_input,\n",
    "                n_output = n_output,\n",
    "                denoising = True,\n",
    "                training_epochs = max_training_epochs,\n",
    "                batch_size = 50,\n",
    "                loss = loss,\n",
    "                train_dir = train_dir,\n",
    "                loss_display_step = 1,\n",
    "                saver_step = 1,\n",
    "                learning_rate = 0.0005,\n",
    "                encoder = enc_dec.encoder_with_convs_and_symmetry,\n",
    "                encoder_args = encoder_args,\n",
    "                decoder = enc_dec.decoder_with_fc_only,\n",
    "                decoder_args = decoder_args\n",
    "               )\n",
    "    \n",
    "    conf.allow_gpu_growth = False\n",
    "    conf.consistent_io = False\n",
    "    conf.experiment_name = experiment_name\n",
    "    conf.save(osp.join(conf.train_dir, 'configuration'))\n",
    "    \n",
    "reset_tf_graph()\n",
    "set_visible_GPUs([GPU])\n",
    "ae = PointNetAutoEncoder(experiment_name, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if do_training:\n",
    "    training_stats = []\n",
    "    training_stats.append(ae.train(train_data, conf))    \n",
    "    with open(osp.join(conf.train_dir, 'train_stats.txt'), 'a') as fout:\n",
    "        np.savetxt(fout, np.array(training_stats)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored in epoch 1.\n",
      "[ 264.4376509]\n",
      "Model restored in epoch 2.\n",
      "[ 252.80357428]\n",
      "Model restored in epoch 4.\n",
      "[ 239.77965602]\n",
      "Model restored in epoch 5.\n",
      "[ 234.79477329]\n",
      "Model restored in epoch 6.\n",
      "[ 233.44128752]\n",
      "Model restored in epoch 7.\n",
      "[ 229.07569884]\n",
      "Model restored in epoch 8.\n",
      "[ 229.2449001]\n",
      "Model restored in epoch 9.\n",
      "[ 228.40404024]\n",
      "Model restored in epoch 10.\n",
      "[ 228.32410398]\n",
      "Model restored in epoch 11.\n",
      "[ 226.75826838]\n",
      "Model restored in epoch 12.\n",
      "[ 224.69203361]\n",
      "Model restored in epoch 13.\n",
      "[ 224.27484073]\n",
      "Model restored in epoch 14.\n",
      "[ 223.96873618]\n",
      "Model restored in epoch 15.\n",
      "[ 222.16463735]\n",
      "Model restored in epoch 16.\n",
      "[ 225.89498777]\n",
      "Model restored in epoch 17.\n",
      "[ 221.63755341]\n",
      "Model restored in epoch 18.\n",
      "[ 220.7238685]\n",
      "Model restored in epoch 19.\n",
      "[ 222.94362773]\n",
      "Model restored in epoch 20.\n",
      "[ 223.73081128]\n",
      "Model restored in epoch 21.\n",
      "[ 220.89370246]\n",
      "Model restored in epoch 22.\n",
      "[ 222.55593116]\n",
      "Model restored in epoch 23.\n",
      "[ 221.64230024]\n",
      "Model restored in epoch 24.\n",
      "[ 220.04207875]\n",
      "Model restored in epoch 25.\n",
      "[ 220.63689139]\n",
      "Model restored in epoch 26.\n",
      "[ 218.21138252]\n",
      "Model restored in epoch 27.\n",
      "[ 218.13565875]\n",
      "Model restored in epoch 28.\n",
      "[ 220.95235618]\n",
      "Model restored in epoch 29.\n",
      "[ 218.77916311]\n",
      "Model restored in epoch 30.\n",
      "[ 219.89713282]\n",
      "Model restored in epoch 31.\n",
      "[ 221.73588727]\n",
      "Model restored in epoch 32.\n",
      "[ 219.32823247]\n",
      "Model restored in epoch 33.\n",
      "[ 217.62967728]\n",
      "Model restored in epoch 34.\n",
      "[ 220.14020469]\n",
      "Model restored in epoch 35.\n",
      "[ 220.25211451]\n",
      "Model restored in epoch 36.\n",
      "[ 217.9088877]\n",
      "Model restored in epoch 37.\n",
      "[ 221.33810979]\n",
      "Model restored in epoch 38.\n",
      "[ 215.60903861]\n",
      "Model restored in epoch 39.\n",
      "[ 219.16104766]\n",
      "Model restored in epoch 40.\n",
      "[ 218.86992631]\n",
      "Model restored in epoch 41.\n",
      "[ 217.69179643]\n",
      "Model restored in epoch 42.\n",
      "[ 218.14027603]\n",
      "Model restored in epoch 43.\n",
      "[ 216.64669191]\n",
      "Model restored in epoch 44.\n",
      "[ 219.13927916]\n",
      "Model restored in epoch 45.\n",
      "[ 217.59049566]\n",
      "Model restored in epoch 46.\n",
      "[ 217.17340429]\n",
      "Model restored in epoch 47.\n",
      "[ 217.73674477]\n",
      "Model restored in epoch 48.\n",
      "[ 216.91285243]\n",
      "Model restored in epoch 49.\n",
      "[ 217.10150991]\n",
      "Model restored in epoch 50.\n",
      "[ 216.40786168]\n",
      "Model restored in epoch 51.\n",
      "[ 216.0612959]\n",
      "Model restored in epoch 52.\n",
      "[ 217.56461229]\n",
      "Model restored in epoch 53.\n",
      "[ 216.87814561]\n",
      "Model restored in epoch 54.\n",
      "[ 217.43190095]\n",
      "Model restored in epoch 55.\n",
      "[ 217.24176127]\n",
      "Model restored in epoch 56.\n",
      "[ 218.475323]\n",
      "Model restored in epoch 57.\n",
      "[ 217.41262847]\n",
      "Model restored in epoch 58.\n",
      "[ 217.97795416]\n",
      "Model restored in epoch 59.\n",
      "[ 216.62117242]\n",
      "Model restored in epoch 60.\n",
      "[ 217.48087334]\n",
      "Model restored in epoch 61.\n",
      "[ 216.77096536]\n",
      "Model restored in epoch 62.\n",
      "[ 216.33414908]\n",
      "Model restored in epoch 63.\n",
      "[ 215.79876162]\n",
      "Model restored in epoch 64.\n",
      "[ 216.89465336]\n",
      "Model restored in epoch 65.\n",
      "[ 215.56021998]\n",
      "Model restored in epoch 66.\n",
      "[ 216.48289833]\n",
      "Model restored in epoch 67.\n",
      "[ 215.87773973]\n",
      "Model restored in epoch 68.\n",
      "[ 215.6961366]\n",
      "Model restored in epoch 69.\n",
      "[ 215.526924]\n",
      "Model restored in epoch 70.\n",
      "[ 216.78154331]\n",
      "Model restored in epoch 71.\n",
      "[ 217.30585002]\n",
      "Model restored in epoch 72.\n",
      "[ 216.03699499]\n",
      "Model restored in epoch 73.\n",
      "[ 215.22402966]\n",
      "Model restored in epoch 74.\n",
      "[ 217.22289028]\n",
      "Model restored in epoch 75.\n",
      "[ 218.08332551]\n",
      "Model restored in epoch 76.\n",
      "[ 216.33667452]\n",
      "Model restored in epoch 77.\n",
      "[ 216.03187576]\n",
      "Model restored in epoch 78.\n",
      "[ 216.41165071]\n",
      "Model restored in epoch 79.\n",
      "[ 216.75044886]\n",
      "Model restored in epoch 80.\n",
      "[ 216.28413417]\n",
      "Model restored in epoch 81.\n",
      "[ 216.32353085]\n",
      "Model restored in epoch 82.\n",
      "[ 214.62098726]\n",
      "Model restored in epoch 83.\n",
      "[ 217.45203519]\n",
      "Model restored in epoch 84.\n",
      "[ 215.05586172]\n",
      "Model restored in epoch 85.\n",
      "[ 215.10919393]\n",
      "Model restored in epoch 86.\n",
      "[ 216.97964301]\n",
      "Model restored in epoch 87.\n",
      "[ 216.96218659]\n",
      "Model restored in epoch 88.\n",
      "[ 215.40469896]\n",
      "Model restored in epoch 89.\n",
      "[ 215.24712206]\n",
      "Model restored in epoch 90.\n",
      "[ 216.51534969]\n",
      "Model restored in epoch 91.\n",
      "[ 216.55890938]\n",
      "Model restored in epoch 92.\n",
      "[ 216.68394695]\n",
      "Model restored in epoch 93.\n",
      "[ 215.69815724]\n",
      "Model restored in epoch 94.\n",
      "[ 215.92305396]\n",
      "Model restored in epoch 95.\n",
      "[ 216.42482665]\n",
      "Model restored in epoch 96.\n",
      "[ 215.961077]\n",
      "Model restored in epoch 97.\n",
      "[ 215.61901891]\n",
      "Model restored in epoch 98.\n",
      "[ 215.11957874]\n",
      "Model restored in epoch 99.\n",
      "[ 218.88389333]\n",
      "Model restored in epoch 100.\n",
      "[ 214.98804245]\n",
      "Best epoch = 82.\n",
      "Test Median-Accuracy-Coverage: 0.837890625 0.86572265625\n"
     ]
    }
   ],
   "source": [
    "if do_evaluation:    \n",
    "    # Pick the epoch that minimizes the loss on the validation dataset.\n",
    "    saved_epochs = np.array(read_saved_epochs(conf.train_dir))\n",
    "    allowable_epochs = saved_epochs[saved_epochs <= max_evaluation_epochs]\n",
    "    val_stats = eval_model(ae, conf, val_data, epochs=allowable_epochs, verbose=True)\n",
    "    val_loss = np.min(val_stats[:,1])\n",
    "    best_epoch = int(val_stats[np.argmin(val_stats[:,1]), 0])\n",
    "    print 'Best epoch = %d.' % (best_epoch,) \n",
    "        \n",
    "    ae.restore_model(conf.train_dir, best_epoch)\n",
    "    top_save_dir = osp.join(conf.train_dir, 'output', 'epoch_' + str(best_epoch))\n",
    "    save_dir = osp.join(top_save_dir, 'test_predictions')\n",
    "    test_recon, test_loss, test_feed, test_ids, test_gt = ae.evaluate(test_data, conf)\n",
    "    save_reconstructions(save_dir, test_recon, test_gt, test_feed, test_ids) # save ply files of test data.    \n",
    "    train_loss = ae.evaluate(train_data, conf)[1]\n",
    "    \n",
    "    # Report Accuracy and Coverage of test data.\n",
    "    n_examples = len(test_recon)\n",
    "    pred_scores = np.zeros((n_examples, 2))\n",
    "    for i in xrange(n_examples):\n",
    "        gt = test_gt[i]\n",
    "        pred = test_recon[i] \n",
    "        pred_scores[i, 0] = accuracy_of_completion(pred, gt, thres=0.02, ret_dists=False)\n",
    "        pred_scores[i, 1] = coverage_of_completion(gt, pred, thres=0.02, ret_dists=False)\n",
    "    \n",
    "    print 'Test Median-Accuracy-Coverage:', np.median(pred_scores[:, 0]), np.median(pred_scores[:, 1])\n",
    "    \n",
    "    save_pc_prediction_stats(osp.join(top_save_dir, 'detailed_stats.txt'), test_ids, pred_scores)\n",
    "    save_stats_of_multi_class_experiments(osp.join(top_save_dir, 'class_stats.txt'), test_ids, pred_scores)\n",
    "    \n",
    "    with open(osp.join(top_save_dir, 'stats.txt'), 'w') as fout:\n",
    "        fout.write('Best Validation Epoch = %d\\n' % (best_epoch))\n",
    "        fout.write('Validation loss = %f\\n' % (val_loss))\n",
    "        fout.write('Train loss = %f\\n' % (train_loss))\n",
    "        fout.write('Test loss = %f\\n' % (test_loss))\n",
    "        fout.write('Gen. Error (abs, per) = %f %f\\n' % (abs(test_loss-train_loss),  abs(test_loss-train_loss) / train_loss ))\n",
    "        fout.write('Test Median-Accuracy-Coverage = %f %f\\n' % (np.median(pred_scores[:, 0]), np.median(pred_scores[:, 1])))\n",
    "        fout.write('Test Median Harmonic Mean = %f' % (np.median(hmean(pred_scores, 1))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
