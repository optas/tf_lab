{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picking GPU 1\n"
     ]
    }
   ],
   "source": [
    "from general_tools.notebook.gpu_utils import setup_one_gpu\n",
    "GPU = 1\n",
    "setup_one_gpu(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "from general_tools.notebook.tf import reset_tf_graph\n",
    "from general_tools.in_out import create_dir\n",
    "from general_tools.in_out.basics import create_dir, delete_files_in_directory, files_in_subdirs\n",
    "\n",
    "from geo_tool import Point_Cloud\n",
    "\n",
    "from tf_lab.in_out.basics import Data_Splitter\n",
    "from tf_lab.point_clouds.ae_templates import mlp_architecture_ala_iclr_18, default_train_params\n",
    "\n",
    "from tf_lab.point_clouds.autoencoder import Configuration as Conf\n",
    "from tf_lab.point_clouds.point_net_ae import PointNetAutoEncoder\n",
    "\n",
    "from tf_lab.point_clouds.in_out import load_point_clouds_from_filenames, PointCloudDataSet\n",
    "from tf_lab.data_sets.shape_net import pc_loader as snc_loader\n",
    "from tf_lab.data_sets.shape_net import snc_category_to_synth_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_data_dir = '/orions4-zfs/projects/optas/DATA/'\n",
    "experiment_tag = 'mlp_with_split_3pc_usampled'\n",
    "loss = 'emd'\n",
    "\n",
    "n_pc_points = 2048\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me the class type: chair\n"
     ]
    }
   ],
   "source": [
    "class_name = raw_input('Give me the class type: ').lower()\n",
    "syn_id = snc_category_to_synth_id()[class_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_multiple_version_of_pcs(version, syn_id):\n",
    "    if version == 'uniform_all':        \n",
    "        versions = ['centered', 'centered_2nd_version', 'centered_3rd_version']    \n",
    "\n",
    "    elif version == 'uniform_one':        \n",
    "        versions = ['centered']\n",
    "        \n",
    "    elif version == 'fps':\n",
    "        versions = ['fps_sampled_in_u_sphere']\n",
    "    \n",
    "    elif version == 'all':\n",
    "        versions = ['centered', 'centered_2nd_version', 'centered_3rd_version', 'fps_sampled_in_u_sphere']\n",
    "        \n",
    "    splits = {'train':None, 'val':None, 'test': None}\n",
    "        \n",
    "    for s in splits.keys():\n",
    "        print 'Loading %s data.' % (s,)\n",
    "        s_file = osp.join(top_data_dir, 'Point_Clouds/Shape_Net/Splits/single_class_splits/'+syn_id+ '/85_5_10/', s + '.txt')\n",
    "        print s_file\n",
    "        for i, v in enumerate(versions):\n",
    "            top_pclouds_path = osp.join(top_data_dir, 'Point_Clouds/Shape_Net/Core/from_manifold_meshes/', v, str(n_pc_points))                \n",
    "            splitter = Data_Splitter(top_pclouds_path, data_file_ending='.ply', random_seed=random_seed)\n",
    "            pcs_in_split = splitter.load_splits(s_file)                \n",
    "            pclouds, model_ids, syn_ids = load_point_clouds_from_filenames(pcs_in_split, n_threads=20, loader=snc_loader, verbose=True)        \n",
    "            if splits[s] is None:\n",
    "                splits[s] = PointCloudDataSet(pclouds, labels=syn_ids + '_' + model_ids)\n",
    "            else:\n",
    "                splits[s].merge(PointCloudDataSet(pclouds, labels=syn_ids + '_' + model_ids))\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data.\n",
      "/orions4-zfs/projects/optas/DATA/Point_Clouds/Shape_Net/Splits/single_class_splits/03001627/85_5_10/test.txt\n",
      "679 pclouds were loaded. They belong in 1 shape-classes.\n",
      "679 pclouds were loaded. They belong in 1 shape-classes.\n",
      "679 pclouds were loaded. They belong in 1 shape-classes.\n",
      "Loading train data.\n",
      "/orions4-zfs/projects/optas/DATA/Point_Clouds/Shape_Net/Splits/single_class_splits/03001627/85_5_10/train.txt\n",
      "5761 pclouds were loaded. They belong in 1 shape-classes.\n",
      "5761 pclouds were loaded. They belong in 1 shape-classes.\n",
      "5761 pclouds were loaded. They belong in 1 shape-classes.\n",
      "Loading val data.\n",
      "/orions4-zfs/projects/optas/DATA/Point_Clouds/Shape_Net/Splits/single_class_splits/03001627/85_5_10/val.txt\n",
      "338 pclouds were loaded. They belong in 1 shape-classes.\n",
      "338 pclouds were loaded. They belong in 1 shape-classes.\n",
      "338 pclouds were loaded. They belong in 1 shape-classes.\n"
     ]
    }
   ],
   "source": [
    "in_data  = load_multiple_version_of_pcs('uniform_all' ,syn_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    batch_size: 50\n",
      "                 consistent_io: None\n",
      "                         debug: False\n",
      "                       decoder: decoder_with_fc_only\n",
      "                  decoder_args: {'b_norm_finish': True, 'verbose': True, 'b_norm': True, 'layer_sizes': [256, 256, 6144]}\n",
      "                       encoder: encoder_with_convs_and_symmetry_new\n",
      "                  encoder_args: {'filter_sizes': [1, 1, 1, 1, 1], 'n_filters': [64, 128, 128, 256, 4], 'verbose': True, 'b_norm': True, 'strides': [1, 1, 1, 1, 1]}\n",
      "                 gauss_augment: None\n",
      "                  is_denoising: False\n",
      "               latent_vs_recon: 1.0\n",
      "                 learning_rate: 0.0005\n",
      "                          loss: emd\n",
      "             loss_display_step: 1\n",
      "                       n_input: [2048, 3]\n",
      "                      n_output: [2048, 3]\n",
      "                           n_z: None\n",
      "             saver_max_to_keep: None\n",
      "                    saver_step: 10\n",
      "                     train_dir: /orions4-zfs/projects/optas/DATA/OUT/iclr/nn_models/ae_chair_mlp_with_split_3pc_usampled_2048_pts_4_bneck_emd\n",
      "               training_epochs: 500\n",
      "                      z_rotate: False\n",
      "\n",
      "Building Encoder\n",
      "encoder_conv_layer_0 conv params =  256 bnorm params =  128\n",
      "Tensor(\"experiment_ae_chair_mlp_with_split_3pc_usampled_2048_pts_4_bneck_emd_1/Relu:0\", shape=(?, 2048, 64), dtype=float32)\n",
      "output size: 131072 \n",
      "\n",
      "encoder_conv_layer_1 conv params =  8320 bnorm params =  256\n",
      "Tensor(\"experiment_ae_chair_mlp_with_split_3pc_usampled_2048_pts_4_bneck_emd_1/Relu_1:0\", shape=(?, 2048, 128), dtype=float32)\n",
      "output size: 262144 \n",
      "\n",
      "encoder_conv_layer_2 conv params =  16512 bnorm params =  256\n",
      "Tensor(\"experiment_ae_chair_mlp_with_split_3pc_usampled_2048_pts_4_bneck_emd_1/Relu_2:0\", shape=(?, 2048, 128), dtype=float32)\n",
      "output size: 262144 \n",
      "\n",
      "encoder_conv_layer_3 conv params =  33024 bnorm params =  512\n",
      "Tensor(\"experiment_ae_chair_mlp_with_split_3pc_usampled_2048_pts_4_bneck_emd_1/Relu_3:0\", shape=(?, 2048, 256), dtype=float32)\n",
      "output size: 524288 \n",
      "\n",
      "encoder_conv_layer_4 conv params =  1028 bnorm params =  8\n",
      "Tensor(\"experiment_ae_chair_mlp_with_split_3pc_usampled_2048_pts_4_bneck_emd_1/Relu_4:0\", shape=(?, 2048, 4), dtype=float32)\n",
      "output size: 8192 \n",
      "\n",
      "Tensor(\"experiment_ae_chair_mlp_with_split_3pc_usampled_2048_pts_4_bneck_emd_1/Max:0\", shape=(?, 4), dtype=float32)\n",
      "Building Decoder\n",
      "decoder_fc_0 FC params =  1280 bnorm params =  512\n",
      "Tensor(\"experiment_ae_chair_mlp_with_split_3pc_usampled_2048_pts_4_bneck_emd_1/Relu_5:0\", shape=(?, 256), dtype=float32)\n",
      "output size: 256 \n",
      "\n",
      "decoder_fc_1 FC params =  65792 bnorm params =  512\n",
      "Tensor(\"experiment_ae_chair_mlp_with_split_3pc_usampled_2048_pts_4_bneck_emd_1/Relu_6:0\", shape=(?, 256), dtype=float32)\n",
      "output size: 256 \n",
      "\n",
      "decoder_fc_2 FC params =  1579008 bnorm params =  12288\n",
      "Tensor(\"experiment_ae_chair_mlp_with_split_3pc_usampled_2048_pts_4_bneck_emd_1/decoder_fc_2_bnorm/batchnorm/add_1:0\", shape=(?, 6144), dtype=float32)\n",
      "output size: 6144 \n",
      "\n",
      "('Epoch:', '0001', 'training time (minutes)=', '0.9550', 'loss=', '1009.570657835')\n",
      "INFO:tensorflow:/orions4-zfs/projects/optas/DATA/OUT/iclr/nn_models/ae_chair_mlp_with_split_3pc_usampled_2048_pts_4_bneck_emd/models.ckpt-1 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "('Epoch:', '0002', 'training time (minutes)=', '0.9691', 'loss=', '776.234803661')\n",
      "('Epoch:', '0003', 'training time (minutes)=', '0.9923', 'loss=', '650.131104754')\n",
      "('Epoch:', '0004', 'training time (minutes)=', '0.9831', 'loss=', '552.932645582')\n",
      "('Epoch:', '0005', 'training time (minutes)=', '0.9878', 'loss=', '428.856145975')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0790', 'loss=', '316.642152786')\n",
      "('Epoch:', '0006', 'training time (minutes)=', '0.9874', 'loss=', '325.480170076')\n",
      "('Epoch:', '0007', 'training time (minutes)=', '0.9934', 'loss=', '215.742085598')\n",
      "('Epoch:', '0008', 'training time (minutes)=', '0.9874', 'loss=', '188.416701651')\n",
      "('Epoch:', '0009', 'training time (minutes)=', '0.9917', 'loss=', '183.946447798')\n",
      "('Epoch:', '0010', 'training time (minutes)=', '0.9894', 'loss=', '180.430627530')\n",
      "INFO:tensorflow:/orions4-zfs/projects/optas/DATA/OUT/iclr/nn_models/ae_chair_mlp_with_split_3pc_usampled_2048_pts_4_bneck_emd/models.ckpt-10 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0787', 'loss=', '180.258793259')\n",
      "('Epoch:', '0011', 'training time (minutes)=', '0.9868', 'loss=', '178.368953451')\n",
      "('Epoch:', '0012', 'training time (minutes)=', '1.0105', 'loss=', '175.365861843')\n",
      "('Epoch:', '0013', 'training time (minutes)=', '0.9936', 'loss=', '174.221793620')\n",
      "('Epoch:', '0014', 'training time (minutes)=', '0.9967', 'loss=', '172.993376093')\n",
      "('Epoch:', '0015', 'training time (minutes)=', '0.9921', 'loss=', '171.984153991')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0787', 'loss=', '170.539326096')\n",
      "('Epoch:', '0016', 'training time (minutes)=', '0.9896', 'loss=', '171.038129989')\n",
      "('Epoch:', '0017', 'training time (minutes)=', '0.9864', 'loss=', '169.864319909')\n",
      "('Epoch:', '0018', 'training time (minutes)=', '0.9937', 'loss=', '168.944175565')\n",
      "('Epoch:', '0019', 'training time (minutes)=', '0.9864', 'loss=', '166.498338075')\n",
      "('Epoch:', '0020', 'training time (minutes)=', '0.9842', 'loss=', '164.403827877')\n",
      "INFO:tensorflow:/orions4-zfs/projects/optas/DATA/OUT/iclr/nn_models/ae_chair_mlp_with_split_3pc_usampled_2048_pts_4_bneck_emd/models.ckpt-20 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0784', 'loss=', '162.599735641')\n",
      "('Epoch:', '0021', 'training time (minutes)=', '0.9826', 'loss=', '162.691221950')\n",
      "('Epoch:', '0022', 'training time (minutes)=', '1.0001', 'loss=', '161.734817903')\n",
      "('Epoch:', '0023', 'training time (minutes)=', '0.9893', 'loss=', '160.433592821')\n",
      "('Epoch:', '0024', 'training time (minutes)=', '0.9948', 'loss=', '160.793264771')\n",
      "('Epoch:', '0025', 'training time (minutes)=', '0.9920', 'loss=', '159.735354791')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0799', 'loss=', '157.351817703')\n",
      "('Epoch:', '0026', 'training time (minutes)=', '0.9920', 'loss=', '159.178951938')\n",
      "('Epoch:', '0027', 'training time (minutes)=', '0.9946', 'loss=', '158.729852030')\n",
      "('Epoch:', '0028', 'training time (minutes)=', '0.9970', 'loss=', '159.326491204')\n",
      "('Epoch:', '0029', 'training time (minutes)=', '0.9882', 'loss=', '159.635119540')\n",
      "('Epoch:', '0030', 'training time (minutes)=', '0.9994', 'loss=', '158.585819676')\n",
      "INFO:tensorflow:/orions4-zfs/projects/optas/DATA/OUT/iclr/nn_models/ae_chair_mlp_with_split_3pc_usampled_2048_pts_4_bneck_emd/models.ckpt-30 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0792', 'loss=', '158.460814285')\n",
      "('Epoch:', '0031', 'training time (minutes)=', '0.9952', 'loss=', '157.994751463')\n",
      "('Epoch:', '0032', 'training time (minutes)=', '0.9956', 'loss=', '158.096187205')\n",
      "('Epoch:', '0033', 'training time (minutes)=', '0.9865', 'loss=', '157.737172047')\n",
      "('Epoch:', '0034', 'training time (minutes)=', '0.9963', 'loss=', '157.567785777')\n",
      "('Epoch:', '0035', 'training time (minutes)=', '0.9989', 'loss=', '157.630909618')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0823', 'loss=', '155.299930573')\n",
      "('Epoch:', '0036', 'training time (minutes)=', '0.9869', 'loss=', '157.349559043')\n",
      "('Epoch:', '0037', 'training time (minutes)=', '1.0052', 'loss=', '157.477061440')\n",
      "('Epoch:', '0038', 'training time (minutes)=', '0.9889', 'loss=', '156.726432026')\n",
      "('Epoch:', '0039', 'training time (minutes)=', '0.9868', 'loss=', '156.909889509')\n",
      "('Epoch:', '0040', 'training time (minutes)=', '0.9896', 'loss=', '156.415496207')\n",
      "INFO:tensorflow:/orions4-zfs/projects/optas/DATA/OUT/iclr/nn_models/ae_chair_mlp_with_split_3pc_usampled_2048_pts_4_bneck_emd/models.ckpt-40 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0788', 'loss=', '155.942277145')\n",
      "('Epoch:', '0041', 'training time (minutes)=', '1.0265', 'loss=', '156.223747983')\n",
      "('Epoch:', '0042', 'training time (minutes)=', '0.9963', 'loss=', '156.426306108')\n",
      "('Epoch:', '0043', 'training time (minutes)=', '0.9898', 'loss=', '156.152894039')\n",
      "('Epoch:', '0044', 'training time (minutes)=', '0.9932', 'loss=', '156.122120733')\n",
      "('Epoch:', '0045', 'training time (minutes)=', '0.9937', 'loss=', '156.173176641')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0800', 'loss=', '156.019406128')\n",
      "('Epoch:', '0046', 'training time (minutes)=', '0.9930', 'loss=', '155.507912545')\n",
      "('Epoch:', '0047', 'training time (minutes)=', '0.9849', 'loss=', '155.070378489')\n",
      "('Epoch:', '0048', 'training time (minutes)=', '0.9964', 'loss=', '155.516851099')\n",
      "('Epoch:', '0049', 'training time (minutes)=', '0.9909', 'loss=', '155.684606182')\n",
      "('Epoch:', '0050', 'training time (minutes)=', '0.9907', 'loss=', '155.233599721')\n",
      "INFO:tensorflow:/orions4-zfs/projects/optas/DATA/OUT/iclr/nn_models/ae_chair_mlp_with_split_3pc_usampled_2048_pts_4_bneck_emd/models.ckpt-50 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0784', 'loss=', '152.743195343')\n",
      "('Epoch:', '0051', 'training time (minutes)=', '0.9894', 'loss=', '155.592595861')\n",
      "('Epoch:', '0052', 'training time (minutes)=', '0.9820', 'loss=', '155.293204531')\n",
      "('Epoch:', '0053', 'training time (minutes)=', '0.9949', 'loss=', '155.182004934')\n",
      "('Epoch:', '0054', 'training time (minutes)=', '0.9782', 'loss=', '155.118313510')\n",
      "('Epoch:', '0055', 'training time (minutes)=', '1.0016', 'loss=', '155.193250042')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0789', 'loss=', '152.697459793')\n",
      "('Epoch:', '0056', 'training time (minutes)=', '0.9858', 'loss=', '154.610240770')\n",
      "('Epoch:', '0057', 'training time (minutes)=', '0.9938', 'loss=', '154.578858704')\n",
      "('Epoch:', '0058', 'training time (minutes)=', '0.9941', 'loss=', '154.778282099')\n",
      "('Epoch:', '0059', 'training time (minutes)=', '0.9977', 'loss=', '154.781392150')\n",
      "('Epoch:', '0060', 'training time (minutes)=', '0.9884', 'loss=', '154.316813195')\n",
      "INFO:tensorflow:/orions4-zfs/projects/optas/DATA/OUT/iclr/nn_models/ae_chair_mlp_with_split_3pc_usampled_2048_pts_4_bneck_emd/models.ckpt-60 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0786', 'loss=', '155.328923035')\n",
      "('Epoch:', '0061', 'training time (minutes)=', '0.9929', 'loss=', '154.674761344')\n",
      "('Epoch:', '0062', 'training time (minutes)=', '1.0013', 'loss=', '154.409042756')\n",
      "('Epoch:', '0063', 'training time (minutes)=', '0.9918', 'loss=', '154.324489870')\n",
      "('Epoch:', '0064', 'training time (minutes)=', '0.9893', 'loss=', '154.121728869')\n",
      "('Epoch:', '0065', 'training time (minutes)=', '0.9953', 'loss=', '153.575958163')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0794', 'loss=', '154.165777588')\n",
      "('Epoch:', '0066', 'training time (minutes)=', '0.9954', 'loss=', '154.080731024')\n",
      "('Epoch:', '0067', 'training time (minutes)=', '0.9946', 'loss=', '153.920770617')\n",
      "('Epoch:', '0068', 'training time (minutes)=', '0.9934', 'loss=', '153.896740457')\n",
      "('Epoch:', '0069', 'training time (minutes)=', '0.9943', 'loss=', '154.030326423')\n",
      "('Epoch:', '0070', 'training time (minutes)=', '1.0007', 'loss=', '153.946993665')\n",
      "INFO:tensorflow:/orions4-zfs/projects/optas/DATA/OUT/iclr/nn_models/ae_chair_mlp_with_split_3pc_usampled_2048_pts_4_bneck_emd/models.ckpt-70 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0799', 'loss=', '152.936831284')\n",
      "('Epoch:', '0071', 'training time (minutes)=', '0.9841', 'loss=', '154.251553831')\n",
      "('Epoch:', '0072', 'training time (minutes)=', '0.9877', 'loss=', '153.579311957')\n",
      "('Epoch:', '0073', 'training time (minutes)=', '0.9848', 'loss=', '153.691364366')\n",
      "('Epoch:', '0074', 'training time (minutes)=', '1.0021', 'loss=', '153.279716646')\n"
     ]
    }
   ],
   "source": [
    "bneck_list = [4, 8, 16, 32, 64, 128]\n",
    "train_params = default_train_params()\n",
    "\n",
    "\n",
    "for bneck in bneck_list:\n",
    "    experiment_id = '_'.join(['ae', class_name, experiment_tag, str(n_pc_points), 'pts', str(bneck), 'bneck', loss])\n",
    "    train_dir = osp.join(top_data_dir, 'OUT/iclr/nn_models/', experiment_id)\n",
    "    create_dir(train_dir)\n",
    "    \n",
    "    reset_tf_graph()\n",
    "    \n",
    "    encoder, decoder, enc_args, dec_args = mlp_architecture_ala_iclr_18(n_pc_points, bneck)\n",
    "    \n",
    "    conf = Conf(n_input = [n_pc_points, 3],\n",
    "                loss = loss,\n",
    "                training_epochs = 500,\n",
    "                batch_size = train_params['batch_size'],\n",
    "                denoising = False,\n",
    "                learning_rate = train_params['learning_rate'],\n",
    "                train_dir = train_dir,\n",
    "                loss_display_step = 1,\n",
    "                saver_step = train_params['saver_step'],\n",
    "                z_rotate = False,\n",
    "                encoder = encoder,\n",
    "                decoder = decoder,\n",
    "                encoder_args = enc_args,\n",
    "                decoder_args = dec_args\n",
    "               )\n",
    "    print conf\n",
    "    conf.experiment_name = 'experiment_' + str(experiment_id)\n",
    "    conf.held_out_step = 5\n",
    "    conf.save(osp.join(train_dir, 'configuration'))\n",
    "    ae = PointNetAutoEncoder(conf.experiment_name, conf)\n",
    "    \n",
    "    buf_size = 1 # flush each line\n",
    "    fout = open(osp.join(conf.train_dir, 'train_stats.txt'), 'a', buf_size)\n",
    "    train_stats = ae.train(in_data['train'], conf, log_file=fout, held_out_data=in_data['test'])\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000749164663748 0.000654067897925\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
