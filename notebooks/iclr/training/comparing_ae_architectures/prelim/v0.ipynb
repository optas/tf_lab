{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picking GPU 0\n"
     ]
    }
   ],
   "source": [
    "from general_tools.notebook.gpu_utils import setup_one_gpu\n",
    "GPU = 0\n",
    "setup_one_gpu(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from general_tools.notebook.tf import reset_tf_graph\n",
    "from general_tools.in_out import create_dir\n",
    "from general_tools.in_out.basics import create_dir, delete_files_in_directory, files_in_subdirs\n",
    "\n",
    "from geo_tool import Point_Cloud\n",
    "\n",
    "from tf_lab.in_out.basics import Data_Splitter\n",
    "from tf_lab.point_clouds.ae_templates import conv_architecture_ala_nips_17, default_train_params_ala_nips_17\n",
    "from tf_lab.point_clouds.encoders_decoders import encoder_with_convs_and_symmetry, decoder_with_fc_only, encoder_with_convs_and_symmetry_new\n",
    "from tf_lab.point_clouds.autoencoder import Configuration as Conf\n",
    "from tf_lab.point_clouds.point_net_ae import PointNetAutoEncoder\n",
    "\n",
    "from tf_lab.point_clouds.in_out import load_point_clouds_from_filenames, PointCloudDataSet\n",
    "from tf_lab.data_sets.shape_net import pc_loader as snc_loader\n",
    "from tf_lab.data_sets.shape_net import snc_category_to_synth_id\n",
    "from tflearn.layers.conv import avg_pool_1d\n",
    "from pc_completions.evaluation import basic_comletion_measures\n",
    "\n",
    "from tflearn.layers.conv import conv_1d\n",
    "from tflearn.layers.normalization import batch_normalization\n",
    "from testing_encoder_decoder import decoder_with_convs_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_pc_points = 2048\n",
    "top_data_dir = '/orions4-zfs/projects/optas/DATA/'\n",
    "top_pclouds_path = osp.join(top_data_dir, 'Point_Clouds/Shape_Net/Core/from_manifold_meshes/centered/', str(n_pc_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_training = True\n",
    "load_pretrained_model = False\n",
    "load_epoch = None\n",
    "random_seed = 42\n",
    "loss = 'chamfer'\n",
    "training_epochs = 250\n",
    "batch_size = 50\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5761 pclouds were loaded. They belong in 1 shape-classes.\n",
      "338 pclouds were loaded. They belong in 1 shape-classes.\n",
      "679 pclouds were loaded. They belong in 1 shape-classes.\n"
     ]
    }
   ],
   "source": [
    "train_split = osp.join(top_data_dir, 'Point_Clouds/Shape_Net/Splits/single_class_splits/03001627/85_5_10/train.txt')\n",
    "val_split = osp.join(top_data_dir, 'Point_Clouds/Shape_Net/Splits/single_class_splits/03001627/85_5_10/val.txt')\n",
    "test_split = osp.join(top_data_dir, 'Point_Clouds/Shape_Net/Splits/single_class_splits/03001627/85_5_10/test.txt')\n",
    "\n",
    "splitter = Data_Splitter(top_pclouds_path, data_file_ending='.ply', random_seed=random_seed)\n",
    "tr_files = splitter.load_splits(train_split)\n",
    "pclouds, model_ids, syn_ids = load_point_clouds_from_filenames(tr_files, n_threads=20, loader=snc_loader, verbose=True)\n",
    "train_data = PointCloudDataSet(pclouds, labels=syn_ids + '_' + model_ids)\n",
    "val_files = splitter.load_splits(val_split)\n",
    "pclouds, model_ids, syn_ids = load_point_clouds_from_filenames(val_files, n_threads=20, loader=snc_loader, verbose=True)\n",
    "val_data = PointCloudDataSet(pclouds, labels=syn_ids + '_' + model_ids)\n",
    "test_files = splitter.load_splits(test_split)\n",
    "pclouds, model_ids, syn_ids = load_point_clouds_from_filenames(test_files, n_threads=20, loader=snc_loader, verbose=True)\n",
    "test_data = PointCloudDataSet(pclouds, labels=syn_ids + '_' + model_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b_norm = True\n",
    "encoder_args = {'n_filters': [64, 128, 128, 256, 512],\n",
    "                'filter_sizes': [9, 3, 2, 2, 1],\n",
    "                'strides': [1, 1, 1, 1, 1],\n",
    "                'b_norm': b_norm,\n",
    "                'pool': avg_pool_1d,\n",
    "                'pool_sizes': [4, None, 2, None, 2],\n",
    "                }\n",
    "\n",
    "\n",
    "decoder_args = {'layer_sizes': [32, 128, 512, 2048*3],\n",
    "                'b_norm': b_norm,\n",
    "                'non_linearity': None,\n",
    "                }\n",
    "\n",
    "# # layer = tf.placeholder(tf.float32, [None, n_pc_points, 3])\n",
    "# # layer = encoder_with_convs_and_symmetry_new(layer, **encoder_args)\n",
    "# # convolutional_decoder(layer)\n",
    "\n",
    "# def encoder_with_convs_and_symmetry_and_fc(in_signal, fc_nout, args_of_patrial={}):\n",
    "#     layer = encoder_with_convs_and_symmetry(in_signal, **args_of_patrial)\n",
    "#     layer = fully_connected(layer, fc_nout, activation='relu', weights_init='xavier')\n",
    "#     return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv params =  1792\n",
      "bnorm params =  128\n",
      "Tensor(\"Test_1/AvgPool1D/Squeeze:0\", shape=(?, 512, 64), dtype=float32) 32768\n",
      "conv params =  24704\n",
      "bnorm params =  256\n",
      "Tensor(\"Test_1/Relu_1:0\", shape=(?, 512, 128), dtype=float32) 65536\n",
      "conv params =  32896\n",
      "bnorm params =  256\n",
      "Tensor(\"Test_1/AvgPool1D_1/Squeeze:0\", shape=(?, 256, 128), dtype=float32) 32768\n",
      "conv params =  65792\n",
      "bnorm params =  512\n",
      "Tensor(\"Test_1/Relu_3:0\", shape=(?, 256, 256), dtype=float32) 65536\n",
      "conv params =  131584\n",
      "bnorm params =  1024\n",
      "Tensor(\"Test_1/AvgPool1D_2/Squeeze:0\", shape=(?, 128, 512), dtype=float32) 65536\n",
      "Tensor(\"Test_1/Max:0\", shape=(?, 512), dtype=float32) 512\n",
      "3498848\n",
      "('Epoch:', '0001', 'training time (minutes)=', '0.0739', 'loss=', '0.011796080')\n",
      "('Epoch:', '0002', 'training time (minutes)=', '0.0633', 'loss=', '0.005358907')\n",
      "('Epoch:', '0003', 'training time (minutes)=', '0.0651', 'loss=', '0.004707753')\n",
      "('Epoch:', '0004', 'training time (minutes)=', '0.0656', 'loss=', '0.004102468')\n",
      "('Epoch:', '0005', 'training time (minutes)=', '0.0745', 'loss=', '0.003784664')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0016', 'loss=', '0.003668794')\n",
      "('Epoch:', '0006', 'training time (minutes)=', '0.0632', 'loss=', '0.003534601')\n",
      "('Epoch:', '0007', 'training time (minutes)=', '0.0629', 'loss=', '0.003480990')\n",
      "('Epoch:', '0008', 'training time (minutes)=', '0.0622', 'loss=', '0.003358079')\n",
      "('Epoch:', '0009', 'training time (minutes)=', '0.0699', 'loss=', '0.003248880')\n",
      "('Epoch:', '0010', 'training time (minutes)=', '0.0676', 'loss=', '0.003125720')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0010', 'loss=', '0.003133877')\n",
      "('Epoch:', '0011', 'training time (minutes)=', '0.0651', 'loss=', '0.003105519')\n",
      "('Epoch:', '0012', 'training time (minutes)=', '0.0642', 'loss=', '0.002989086')\n",
      "('Epoch:', '0013', 'training time (minutes)=', '0.0711', 'loss=', '0.002877961')\n",
      "('Epoch:', '0014', 'training time (minutes)=', '0.0640', 'loss=', '0.002869558')\n",
      "('Epoch:', '0015', 'training time (minutes)=', '0.0675', 'loss=', '0.002901316')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0011', 'loss=', '0.003470875')\n",
      "('Epoch:', '0016', 'training time (minutes)=', '0.0635', 'loss=', '0.002831518')\n",
      "('Epoch:', '0017', 'training time (minutes)=', '0.0658', 'loss=', '0.002702802')\n",
      "('Epoch:', '0018', 'training time (minutes)=', '0.0628', 'loss=', '0.002808708')\n",
      "('Epoch:', '0019', 'training time (minutes)=', '0.0646', 'loss=', '0.002638608')\n",
      "('Epoch:', '0020', 'training time (minutes)=', '0.0661', 'loss=', '0.002602868')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0009', 'loss=', '0.002862934')\n",
      "('Epoch:', '0021', 'training time (minutes)=', '0.0626', 'loss=', '0.002605332')\n",
      "('Epoch:', '0022', 'training time (minutes)=', '0.0637', 'loss=', '0.002510141')\n",
      "('Epoch:', '0023', 'training time (minutes)=', '0.0633', 'loss=', '0.002508912')\n",
      "('Epoch:', '0024', 'training time (minutes)=', '0.0690', 'loss=', '0.002474068')\n",
      "('Epoch:', '0025', 'training time (minutes)=', '0.0624', 'loss=', '0.002485643')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0009', 'loss=', '0.002485432')\n",
      "('Epoch:', '0026', 'training time (minutes)=', '0.0637', 'loss=', '0.002352525')\n",
      "('Epoch:', '0027', 'training time (minutes)=', '0.0683', 'loss=', '0.002300747')\n",
      "('Epoch:', '0028', 'training time (minutes)=', '0.0641', 'loss=', '0.002337761')\n",
      "('Epoch:', '0029', 'training time (minutes)=', '0.0664', 'loss=', '0.002256381')\n",
      "('Epoch:', '0030', 'training time (minutes)=', '0.0712', 'loss=', '0.002246263')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0009', 'loss=', '0.002397475')\n",
      "('Epoch:', '0031', 'training time (minutes)=', '0.0674', 'loss=', '0.002227749')\n",
      "('Epoch:', '0032', 'training time (minutes)=', '0.0640', 'loss=', '0.002232592')\n",
      "('Epoch:', '0033', 'training time (minutes)=', '0.0632', 'loss=', '0.002137282')\n",
      "('Epoch:', '0034', 'training time (minutes)=', '0.0634', 'loss=', '0.002124662')\n",
      "('Epoch:', '0035', 'training time (minutes)=', '0.0624', 'loss=', '0.002093956')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0009', 'loss=', '0.002329384')\n",
      "('Epoch:', '0036', 'training time (minutes)=', '0.0666', 'loss=', '0.002082005')\n",
      "('Epoch:', '0037', 'training time (minutes)=', '0.0692', 'loss=', '0.002021437')\n",
      "('Epoch:', '0038', 'training time (minutes)=', '0.0642', 'loss=', '0.002018250')\n",
      "('Epoch:', '0039', 'training time (minutes)=', '0.0650', 'loss=', '0.001994274')\n",
      "('Epoch:', '0040', 'training time (minutes)=', '0.0627', 'loss=', '0.001967339')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0009', 'loss=', '0.001937275')\n",
      "('Epoch:', '0041', 'training time (minutes)=', '0.0644', 'loss=', '0.001946763')\n",
      "('Epoch:', '0042', 'training time (minutes)=', '0.0639', 'loss=', '0.001905685')\n",
      "('Epoch:', '0043', 'training time (minutes)=', '0.0644', 'loss=', '0.001881872')\n",
      "('Epoch:', '0044', 'training time (minutes)=', '0.0634', 'loss=', '0.001834340')\n",
      "('Epoch:', '0045', 'training time (minutes)=', '0.0634', 'loss=', '0.001798399')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0009', 'loss=', '0.001831852')\n",
      "('Epoch:', '0046', 'training time (minutes)=', '0.0617', 'loss=', '0.001823202')\n",
      "('Epoch:', '0047', 'training time (minutes)=', '0.0680', 'loss=', '0.001745700')\n",
      "('Epoch:', '0048', 'training time (minutes)=', '0.0664', 'loss=', '0.001705360')\n",
      "('Epoch:', '0049', 'training time (minutes)=', '0.0646', 'loss=', '0.001708114')\n"
     ]
    }
   ],
   "source": [
    "train_dir = osp.join(top_data_dir, 'OUT/iclr/nn_models/testing_ae_settings/temp')\n",
    "\n",
    "conf = Conf(\n",
    "            n_input = [n_pc_points, 3],\n",
    "            loss = loss,\n",
    "            training_epochs = training_epochs,\n",
    "            batch_size = batch_size,\n",
    "            denoising = False,\n",
    "            learning_rate = learning_rate,\n",
    "            train_dir = train_dir,             \n",
    "            loss_display_step = 1,\n",
    "            saver_step = None,\n",
    "            z_rotate = False,\n",
    "            encoder = encoder_with_convs_and_symmetry_new, \n",
    "            decoder = decoder_with_fc_only,\n",
    "            encoder_args = encoder_args,\n",
    "            decoder_args = decoder_args\n",
    "           )\n",
    "\n",
    "conf.experiment_name = 'Test'\n",
    "conf.held_out_step = 5\n",
    "reset_tf_graph()\n",
    "ae = PointNetAutoEncoder(conf.experiment_name, conf)\n",
    "\n",
    "print ae.trainable_parameters()\n",
    "\n",
    "if do_training:\n",
    "    buf_size = 1 # flush each line\n",
    "    fout = open(osp.join(conf.train_dir, 'train_stats.txt'), 'a', buf_size)    \n",
    "    train_stats = ae.train(train_data, conf, log_file=fout, held_out_data=val_data)\n",
    "    fout.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
