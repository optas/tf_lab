{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picking GPU 0\n"
     ]
    }
   ],
   "source": [
    "from general_tools.notebook.gpu_utils import setup_one_gpu\n",
    "GPU = 0\n",
    "setup_one_gpu(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "from general_tools.notebook.tf import reset_tf_graph\n",
    "from general_tools.in_out import create_dir\n",
    "from general_tools.in_out.basics import create_dir, delete_files_in_directory, files_in_subdirs\n",
    "\n",
    "from geo_tool import Point_Cloud\n",
    "\n",
    "from tf_lab.in_out.basics import Data_Splitter\n",
    "from tf_lab.point_clouds.ae_templates import conv_architecture_ala_nips_17, default_train_params_ala_nips_17\n",
    "\n",
    "from tf_lab.point_clouds.encoders_decoders import decoder_with_fc_only,\\\n",
    "encoder_with_convs_and_symmetry_new, decoder_with_convs_only\n",
    "\n",
    "from tf_lab.point_clouds.autoencoder import Configuration as Conf\n",
    "from tf_lab.point_clouds.point_net_ae import PointNetAutoEncoder\n",
    "\n",
    "from tf_lab.point_clouds.in_out import load_point_clouds_from_filenames, PointCloudDataSet\n",
    "from tf_lab.data_sets.shape_net import pc_loader as snc_loader\n",
    "from tf_lab.data_sets.shape_net import snc_category_to_synth_id\n",
    "from tflearn.layers.conv import avg_pool_1d\n",
    "from pc_completions.evaluation import basic_comletion_measures\n",
    "\n",
    "from tflearn.layers.conv import conv_1d, highway_conv_1d, avg_pool_1d, max_pool_1d\n",
    "from tf_lab.fundamentals.layers import conv_1d_tranpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_pc_points = 2048\n",
    "top_data_dir = '/orions4-zfs/projects/optas/DATA/'\n",
    "top_pclouds_path = osp.join(top_data_dir, 'Point_Clouds/Shape_Net/Core/from_manifold_meshes/centered/', str(n_pc_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_training = True\n",
    "load_pretrained_model = False\n",
    "load_epoch = None\n",
    "random_seed = 42\n",
    "loss = 'chamfer'\n",
    "training_epochs = 2000\n",
    "batch_size = 50\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5761 pclouds were loaded. They belong in 1 shape-classes.\n",
      "338 pclouds were loaded. They belong in 1 shape-classes.\n",
      "679 pclouds were loaded. They belong in 1 shape-classes.\n"
     ]
    }
   ],
   "source": [
    "train_split = osp.join(top_data_dir, 'Point_Clouds/Shape_Net/Splits/single_class_splits/03001627/85_5_10/train.txt')\n",
    "val_split = osp.join(top_data_dir, 'Point_Clouds/Shape_Net/Splits/single_class_splits/03001627/85_5_10/val.txt')\n",
    "test_split = osp.join(top_data_dir, 'Point_Clouds/Shape_Net/Splits/single_class_splits/03001627/85_5_10/test.txt')\n",
    "\n",
    "splitter = Data_Splitter(top_pclouds_path, data_file_ending='.ply', random_seed=random_seed)\n",
    "tr_files = splitter.load_splits(train_split)\n",
    "pclouds, model_ids, syn_ids = load_point_clouds_from_filenames(tr_files, n_threads=20, loader=snc_loader, verbose=True)\n",
    "train_data = PointCloudDataSet(pclouds, labels=syn_ids + '_' + model_ids)\n",
    "val_files = splitter.load_splits(val_split)\n",
    "pclouds, model_ids, syn_ids = load_point_clouds_from_filenames(val_files, n_threads=20, loader=snc_loader, verbose=True)\n",
    "val_data = PointCloudDataSet(pclouds, labels=syn_ids + '_' + model_ids)\n",
    "test_files = splitter.load_splits(test_split)\n",
    "pclouds, model_ids, syn_ids = load_point_clouds_from_filenames(test_files, n_threads=20, loader=snc_loader, verbose=True)\n",
    "test_data = PointCloudDataSet(pclouds, labels=syn_ids + '_' + model_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enc_dec_templates(exp_id):\n",
    "    if exp_id == 0:\n",
    "        encoder = encoder_with_convs_and_symmetry_new\n",
    "        decoder = decoder_with_convs_only\n",
    "\n",
    "        encoder_args = {'n_filters': [64, 128, 128, 256, 32],\n",
    "                        'filter_sizes': [9, 3, 2, 2, 2],\n",
    "                        'strides': [3, 2, 1, 1, 1],\n",
    "                        'b_norm': b_norm,\n",
    "                        'pool': avg_pool_1d,\n",
    "                        'pool_sizes': [6, 4, 4, 2, 2],\n",
    "                        'verbose': True,\n",
    "                        'symmetry': None\n",
    "                    }\n",
    "\n",
    "        decoder_args = {'n_filters': [128, 128, 64, 64, 3],\n",
    "                        'filter_sizes': [2, 3, 3, 3, 6],\n",
    "                        'strides': [2, 4, 4, 8, 8],\n",
    "                        'b_norm': b_norm,\n",
    "                        'non_linearity': None,\n",
    "                        'conv_op': deconv_op,\n",
    "                        'verbose': True\n",
    "                    }\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Encoder\n",
      "encoder_conv_layer_0 conv params =  1792 bnorm params =  128\n",
      "Tensor(\"AvgPool1D_15/Squeeze:0\", shape=(?, 114, 64), dtype=float32)\n",
      "output size: 7296 \n",
      "\n",
      "encoder_conv_layer_1 conv params =  24704 bnorm params =  256\n",
      "Tensor(\"AvgPool1D_16/Squeeze:0\", shape=(?, 15, 128), dtype=float32)\n",
      "output size: 1920 \n",
      "\n",
      "encoder_conv_layer_2 conv params =  32896 bnorm params =  256\n",
      "Tensor(\"AvgPool1D_17/Squeeze:0\", shape=(?, 4, 128), dtype=float32)\n",
      "output size: 512 \n",
      "\n",
      "encoder_conv_layer_3 conv params =  65792 bnorm params =  512\n",
      "Tensor(\"AvgPool1D_18/Squeeze:0\", shape=(?, 2, 256), dtype=float32)\n",
      "output size: 512 \n",
      "\n",
      "encoder_conv_layer_4 conv params =  16416 bnorm params =  64\n",
      "Tensor(\"AvgPool1D_19/Squeeze:0\", shape=(?, 1, 32), dtype=float32)\n",
      "output size: 32 \n",
      "\n",
      "Building Decoder\n",
      "decoder_conv_layer_0 conv params =  8320 bnorm params =  256\n",
      "Tensor(\"decoder_conv_layer_0_bnorm/batchnorm/add_1:0\", shape=(50, 2, 128), dtype=float32)\n",
      "output size: 256 \n",
      "\n",
      "decoder_conv_layer_1 conv params =  49280 bnorm params =  256\n",
      "Tensor(\"decoder_conv_layer_1_bnorm/batchnorm/add_1:0\", shape=(50, 8, 128), dtype=float32)\n",
      "output size: 1024 \n",
      "\n",
      "decoder_conv_layer_2 conv params =  24640 bnorm params =  128\n",
      "Tensor(\"decoder_conv_layer_2_bnorm/batchnorm/add_1:0\", shape=(50, 32, 64), dtype=float32)\n",
      "output size: 2048 \n",
      "\n",
      "decoder_conv_layer_3 conv params =  12352 bnorm params =  128\n",
      "Tensor(\"decoder_conv_layer_3_bnorm/batchnorm/add_1:0\", shape=(50, 256, 64), dtype=float32)\n",
      "output size: 16384 \n",
      "\n",
      "decoder_conv_layer_4 conv params =  1155 Tensor(\"decoder_conv_layer_4/BiasAdd:0\", shape=(50, 2048, 3), dtype=float32)\n",
      "output size: 6144 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# decoder = decoder_with_fc_only\n",
    "b_norm = True\n",
    "\n",
    "\n",
    "\n",
    "# deconv_op = conv_1d\n",
    "deconv_op = partial(conv_1d_tranpose, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# decoder_args = {'layer_sizes': [32, 128, 128, 2048*3],\n",
    "#                 'b_norm': b_norm,\n",
    "#                 'b_norm_finish': True,\n",
    "#                 'non_linearity': None,\n",
    "#                 'verbose': True\n",
    "#                 }\n",
    "\n",
    "\n",
    "layer = tf.placeholder(tf.float32, [None, n_pc_points, 3])\n",
    "layer = encoder(layer, **encoder_args)\n",
    "layer = decoder(layer, **decoder_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Encoder\n",
      "encoder_conv_layer_0 conv params =  1792 bnorm params =  128\n",
      "Tensor(\"Test_1/Relu:0\", shape=(?, 683, 64), dtype=float32)\n",
      "output size: 43712 \n",
      "\n",
      "encoder_conv_layer_1 conv params =  24704 bnorm params =  256\n",
      "Tensor(\"Test_1/Relu_1:0\", shape=(?, 342, 128), dtype=float32)\n",
      "output size: 43776 \n",
      "\n",
      "encoder_conv_layer_2 conv params =  32896 bnorm params =  256\n",
      "Tensor(\"Test_1/Relu_2:0\", shape=(?, 342, 128), dtype=float32)\n",
      "output size: 43776 \n",
      "\n",
      "encoder_conv_layer_3 conv params =  65792 bnorm params =  512\n",
      "Tensor(\"Test_1/Relu_3:0\", shape=(?, 342, 256), dtype=float32)\n",
      "output size: 87552 \n",
      "\n",
      "encoder_conv_layer_4 conv params =  16416 bnorm params =  64\n",
      "Tensor(\"Test_1/Relu_4:0\", shape=(?, 342, 32), dtype=float32)\n",
      "output size: 10944 \n",
      "\n",
      "Building Decoder\n",
      "decoder_fc_0 FC params =  350240 bnorm params =  64\n",
      "Tensor(\"Test_1/decoder_fc_0_bnorm/batchnorm/add_1:0\", shape=(?, 32), dtype=float32)\n",
      "output size: 32 \n",
      "\n",
      "decoder_fc_1 FC params =  4224 bnorm params =  256\n",
      "Tensor(\"Test_1/decoder_fc_1_bnorm/batchnorm/add_1:0\", shape=(?, 128), dtype=float32)\n",
      "output size: 128 \n",
      "\n",
      "decoder_fc_2 FC params =  16512 bnorm params =  256\n",
      "Tensor(\"Test_1/decoder_fc_2_bnorm/batchnorm/add_1:0\", shape=(?, 128), dtype=float32)\n",
      "output size: 128 \n",
      "\n",
      "decoder_fc_3 FC params =  792576 bnorm params =  12288\n",
      "Tensor(\"Test_1/decoder_fc_3_bnorm/batchnorm/add_1:0\", shape=(?, 6144), dtype=float32)\n",
      "output size: 6144 \n",
      "\n",
      "1319232\n"
     ]
    }
   ],
   "source": [
    "train_dir = osp.join(top_data_dir, 'OUT/iclr/nn_models/testing_ae_settings/temp')\n",
    "\n",
    "conf = Conf(\n",
    "            n_input = [n_pc_points, 3],\n",
    "            loss = loss,\n",
    "            training_epochs = training_epochs,\n",
    "            batch_size = batch_size,\n",
    "            denoising = False,\n",
    "            learning_rate = learning_rate,\n",
    "            train_dir = train_dir,             \n",
    "            loss_display_step = 1,\n",
    "            saver_step = None,\n",
    "            z_rotate = False,\n",
    "            encoder = encoder, \n",
    "            decoder = decoder,\n",
    "            encoder_args = encoder_args,\n",
    "            decoder_args = decoder_args\n",
    "           )\n",
    "\n",
    "conf.experiment_name = 'Test'\n",
    "conf.held_out_step = 5\n",
    "reset_tf_graph()\n",
    "ae = PointNetAutoEncoder(conf.experiment_name, conf)\n",
    "\n",
    "print ae.trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', '0001', 'training time (minutes)=', '0.0578', 'loss=', '2.232838356')\n",
      "('Epoch:', '0002', 'training time (minutes)=', '0.0508', 'loss=', '1.542780295')\n",
      "('Epoch:', '0003', 'training time (minutes)=', '0.0516', 'loss=', '1.234779115')\n",
      "('Epoch:', '0004', 'training time (minutes)=', '0.0585', 'loss=', '1.046120163')\n",
      "('Epoch:', '0005', 'training time (minutes)=', '0.0507', 'loss=', '0.884027935')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0019', 'loss=', '0.820954820')\n",
      "('Epoch:', '0006', 'training time (minutes)=', '0.0506', 'loss=', '0.743712132')\n",
      "('Epoch:', '0007', 'training time (minutes)=', '0.0475', 'loss=', '0.620006267')\n",
      "('Epoch:', '0008', 'training time (minutes)=', '0.0567', 'loss=', '0.512350684')\n",
      "('Epoch:', '0009', 'training time (minutes)=', '0.0496', 'loss=', '0.419372054')\n",
      "('Epoch:', '0010', 'training time (minutes)=', '0.0501', 'loss=', '0.341981055')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0008', 'loss=', '0.307026476')\n",
      "('Epoch:', '0011', 'training time (minutes)=', '0.0495', 'loss=', '0.274358338')\n",
      "('Epoch:', '0012', 'training time (minutes)=', '0.0541', 'loss=', '0.219698307')\n",
      "('Epoch:', '0013', 'training time (minutes)=', '0.0541', 'loss=', '0.172972402')\n",
      "('Epoch:', '0014', 'training time (minutes)=', '0.0526', 'loss=', '0.136513520')\n",
      "('Epoch:', '0015', 'training time (minutes)=', '0.0531', 'loss=', '0.106311671')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0008', 'loss=', '0.093761832')\n",
      "('Epoch:', '0016', 'training time (minutes)=', '0.0529', 'loss=', '0.082703362')\n",
      "('Epoch:', '0017', 'training time (minutes)=', '0.0497', 'loss=', '0.063435720')\n",
      "('Epoch:', '0018', 'training time (minutes)=', '0.0531', 'loss=', '0.049007100')\n",
      "('Epoch:', '0019', 'training time (minutes)=', '0.0509', 'loss=', '0.037563432')\n",
      "('Epoch:', '0020', 'training time (minutes)=', '0.0508', 'loss=', '0.029056441')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0008', 'loss=', '0.025848815')\n",
      "('Epoch:', '0021', 'training time (minutes)=', '0.0508', 'loss=', '0.022871819')\n",
      "('Epoch:', '0022', 'training time (minutes)=', '0.0562', 'loss=', '0.018231137')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-5ad8e0ef1579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mbuf_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# flush each line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_stats.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheld_out_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/orions4-zfs/projects/optas/Git_Repos/tf_lab/point_clouds/autoencoder.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data, configuration, log_file, held_out_data)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_single_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/orions4-zfs/projects/optas/Git_Repos/tf_lab/point_clouds/point_net_ae.pyc\u001b[0m in \u001b[0;36m_single_epoch_train\u001b[0;34m(self, train_data, configuration, only_fw)\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;31m# Compute average loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/orions4-zfs/projects/optas/Git_Repos/tf_lab/point_clouds/autoencoder.pyc\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, GT)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_reconstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_reconstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mis_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if do_training:\n",
    "    buf_size = 1 # flush each line\n",
    "    fout = open(osp.join(conf.train_dir, 'train_stats.txt'), 'a', buf_size)    \n",
    "    train_stats = ae.train(train_data, conf, log_file=fout, held_out_data=val_data)\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.00070632771366154368, 3.2764317989349365)\n",
      "0.000619403164179\n",
      "(0.00070817585447398215, 2.84295392036438)\n",
      "0.00061691453293\n",
      "(0.00071282371392716534, 2.72479510307312)\n",
      "0.000601568156261\n",
      "(0.0007123221688822884, 2.686289072036743)\n",
      "0.000614956626907\n",
      "(0.0007036546918401576, 2.905812978744507)\n",
      "0.000621547584764\n",
      "(0.00070395093137884271, 2.658350944519043)\n",
      "0.00060599925454\n",
      "(0.00070694156254277277, 2.7218101024627686)\n",
      "0.000599984718904\n",
      "(0.00070774137578985617, 3.156014919281006)\n",
      "0.000612763098718\n",
      "(0.0007164171765274976, 2.7741189002990723)\n",
      "0.00060548167607\n",
      "(0.00070373827146123281, 2.814279079437256)\n",
      "0.000609081925007\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print ae._single_epoch_train(train_data, conf)\n",
    "    print reconstruct_pclouds(ae, train_data.point_clouds, 510, compute_loss=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000609081949913\n"
     ]
    }
   ],
   "source": [
    "print reconstruct_pclouds(ae, train_data.point_clouds, 510, compute_loss=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(train_data.num_examples / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115.22"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.num_examples / float(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000612877637805\n"
     ]
    }
   ],
   "source": [
    "# from tf_lab.point_clouds.convenience import reconstruct_pclouds\n",
    "\n",
    "# _, l = reconstruct_pclouds(ae, train_data.point_clouds, 1, compute_loss=True)\n",
    "# print l\n",
    "\n",
    "_, l = reconstruct_pclouds(ae, train_data.point_clouds, 33, compute_loss=True)\n",
    "print l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# decoder_args = {'n_filters': [6, 6, 12, 12, 3],\n",
    "#                 'filter_sizes': [9, 3, 2, 2, 1],\n",
    "#                 'strides': [8, 4, 4, 2, 2],\n",
    "#                 'b_norm': b_norm,\n",
    "#                 'non_linearity': tf.nn.relu,\n",
    "#                 'b_norm_finish': True,\n",
    "#                 'conv_op': conv_1d,\n",
    "#                 'verbose': True,\n",
    "#                 'padding': 'valid'\n",
    "#                 }\n",
    "\n",
    "# b_norm = True\n",
    "# encoder_args = {'n_filters': [64, 128, 128, 256],\n",
    "#                 'filter_sizes': [9, 3, 2, 2],\n",
    "#                 'strides': [1, 1, 1, 1],\n",
    "#                 'b_norm': b_norm,\n",
    "#                 'pool': avg_pool_1d,\n",
    "#                 'pool_sizes': [2, None, 2, None],\n",
    "#                 'symmetry': None\n",
    "#                 }\n",
    "\n",
    "# decoder_args = {'n_filters': [6, 6, 12, 12],\n",
    "#                 'filter_sizes': [2, 2, 4, 8],\n",
    "#                 'strides': [1, 1, 1, 1],\n",
    "#                 'b_norm': b_norm,\n",
    "#                 'non_linearity': tf.nn.relu,\n",
    "#                 'b_norm_finish': True,\n",
    "#                 'conv_op': conv_1d,\n",
    "#                 'verbose': True\n",
    "#                 }\n",
    "\n",
    "\n",
    "# encoder_args = {'n_filters': [64, 64, 128, 128, 256, 256, 32],\n",
    "#                 'filter_sizes': [9, 8, 8, 5, 3, 3, 2],\n",
    "#                 'strides': [1, 1, 1, 1, 1, 1, 1],\n",
    "#                 'b_norm': b_norm,\n",
    "#                 'pool': avg_pool_1d,\n",
    "#                 'pool_sizes': [4, 4, 4, 4, 2, 2, 2],\n",
    "#                 'symmetry': None,\n",
    "#                 'padding': 'same'\n",
    "#                 }\n",
    "\n",
    "# decoder_args = {'n_filters': [256, 128, 128, 64, 64, 3],\n",
    "#                 'filter_sizes': [3, 3, 5, 5, 5, 6],\n",
    "#                 'strides': [2, 2, 4, 4, 4, 8],\n",
    "#                 'b_norm': b_norm,\n",
    "#                 'non_linearity': tf.nn.relu,\n",
    "#                 'b_norm_finish': False,\n",
    "#                 'conv_op': deconv_op\n",
    "#                 }\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
