{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picking GPU 1\n"
     ]
    }
   ],
   "source": [
    "from general_tools.notebook.gpu_utils import setup_one_gpu\n",
    "GPU = 1\n",
    "setup_one_gpu(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "from general_tools.notebook.tf import reset_tf_graph\n",
    "from general_tools.in_out import create_dir\n",
    "from general_tools.in_out.basics import create_dir, delete_files_in_directory, files_in_subdirs\n",
    "\n",
    "from geo_tool import Point_Cloud\n",
    "\n",
    "from tf_lab.in_out.basics import Data_Splitter\n",
    "from tf_lab.point_clouds.ae_templates import conv_architecture_ala_nips_17, default_train_params_ala_nips_17\n",
    "\n",
    "from tf_lab.point_clouds.encoders_decoders import decoder_with_fc_only,\\\n",
    "encoder_with_convs_and_symmetry_new, decoder_with_convs_only\n",
    "\n",
    "from tf_lab.point_clouds.autoencoder import Configuration as Conf\n",
    "from tf_lab.point_clouds.point_net_ae import PointNetAutoEncoder\n",
    "\n",
    "from tf_lab.point_clouds.in_out import load_point_clouds_from_filenames, PointCloudDataSet\n",
    "from tf_lab.data_sets.shape_net import pc_loader as snc_loader\n",
    "from tf_lab.data_sets.shape_net import snc_category_to_synth_id\n",
    "from tflearn.layers.conv import avg_pool_1d\n",
    "from pc_completions.evaluation import basic_comletion_measures\n",
    "\n",
    "from tflearn.layers.conv import conv_1d, highway_conv_1d, avg_pool_1d, max_pool_1d\n",
    "from tf_lab.fundamentals.layers import conv_1d_tranpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_id = -1\n",
    "n_pc_points = 2048\n",
    "random_seed = 42\n",
    "top_data_dir = '/orions4-zfs/projects/optas/DATA'\n",
    "train_dir = osp.join(top_data_dir, 'OUT/iclr/nn_models/testing_ae_settings/ae_variants_2', str(exp_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_multiple_version_of_pcs():\n",
    "#     versions = ['centered', 'centered_2nd_version', 'centered_3rd_version']\n",
    "#     versions = ['fps_sampled_in_u_sphere']    \n",
    "    versions = ['fps_sampled_in_u_sphere_fiedler_sorted']\n",
    "    \n",
    "    splits = {'train':None, 'val':None, 'test': None}\n",
    "        \n",
    "    for s in splits.keys():\n",
    "        print 'Loading %s data.' % (s,)\n",
    "        s_file = osp.join(top_data_dir, 'Point_Clouds/Shape_Net/Splits/single_class_splits/03001627/85_5_10/', s + '.txt')\n",
    "        print s_file\n",
    "        for i, v in enumerate(versions):\n",
    "            top_pclouds_path = osp.join(top_data_dir, 'Point_Clouds/Shape_Net/Core/from_manifold_meshes/', v, str(n_pc_points))                \n",
    "            splitter = Data_Splitter(top_pclouds_path, data_file_ending='.ply', random_seed=random_seed)\n",
    "            pcs_in_split = splitter.load_splits(s_file)                \n",
    "            pclouds, model_ids, syn_ids = load_point_clouds_from_filenames(pcs_in_split, n_threads=20, loader=snc_loader, verbose=True)        \n",
    "            if splits[s] is None:\n",
    "                splits[s] = PointCloudDataSet(pclouds, labels=syn_ids + '_' + model_ids)\n",
    "            else:\n",
    "                splits[s].merge(PointCloudDataSet(pclouds, labels=syn_ids + '_' + model_ids))\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data.\n",
      "/orions4-zfs/projects/optas/DATA/Point_Clouds/Shape_Net/Splits/single_class_splits/03001627/85_5_10/test.txt\n",
      "679 pclouds were loaded. They belong in 1 shape-classes.\n",
      "Loading train data.\n",
      "/orions4-zfs/projects/optas/DATA/Point_Clouds/Shape_Net/Splits/single_class_splits/03001627/85_5_10/train.txt\n",
      "5761 pclouds were loaded. They belong in 1 shape-classes.\n",
      "Loading val data.\n",
      "/orions4-zfs/projects/optas/DATA/Point_Clouds/Shape_Net/Splits/single_class_splits/03001627/85_5_10/val.txt\n",
      "338 pclouds were loaded. They belong in 1 shape-classes.\n"
     ]
    }
   ],
   "source": [
    "in_data  = load_multiple_version_of_pcs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_training = True\n",
    "load_pretrained_model = False\n",
    "load_epoch = None\n",
    "loss = 'chamfer'\n",
    "training_epochs = 500\n",
    "batch_size = 50\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def enc_dec_templates():\n",
    "    b_norm = True        \n",
    "    encoder = encoder_with_convs_and_symmetry_new\n",
    "    decoder = decoder_with_fc_only\n",
    "    conv_op = conv_1d\n",
    "\n",
    "    encoder_args = {'n_filters': [16, 32, 64, 128, 2048],\n",
    "            'filter_sizes': [2, 2, 2, 2, 2],\n",
    "            'strides': [1, 1, 1, 1, 1],\n",
    "            'b_norm': b_norm,\n",
    "            'pool': None,\n",
    "            'pool_sizes': [2, None, 2, None, None],\n",
    "            'verbose': True,\n",
    "            'conv_op': conv_op,\n",
    "            }\n",
    "\n",
    "    decoder_args= {'layer_sizes': [32, 256, 2048*3],\n",
    "                   'verbose': True,\n",
    "                   'b_norm': b_norm,\n",
    "                   'non_linearity': tf.nn.relu\n",
    "                  }\n",
    "    return encoder, decoder, encoder_args, decoder_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filter_sizes': [2, 2, 2, 2, 2], 'n_filters': [16, 32, 64, 128, 2048], 'verbose': True, 'strides': [1, 1, 1, 1, 1], 'conv_op': <function conv_1d at 0x7f2592c26320>, 'pool_sizes': [2, None, 2, None, None], 'b_norm': True, 'pool': None}\n"
     ]
    }
   ],
   "source": [
    "reset_tf_graph()\n",
    "encoder, decoder, encoder_args, decoder_args = enc_dec_templates()\n",
    "print encoder_args\n",
    "\n",
    "# layer = tf.placeholder(tf.float32, [None, n_pc_points, 3])\n",
    "# layer = encoder(layer, **encoder_args)\n",
    "# layer = decoder(layer, **decoder_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Encoder\n",
      "encoder_conv_layer_0 conv params =  112 bnorm params =  32\n",
      "Tensor(\"experiment_-1_1/Relu:0\", shape=(?, 2048, 16), dtype=float32)\n",
      "output size: 32768 \n",
      "\n",
      "encoder_conv_layer_1 conv params =  1056 bnorm params =  64\n",
      "Tensor(\"experiment_-1_1/Relu_1:0\", shape=(?, 2048, 32), dtype=float32)\n",
      "output size: 65536 \n",
      "\n",
      "encoder_conv_layer_2 conv params =  4160 bnorm params =  128\n",
      "Tensor(\"experiment_-1_1/Relu_2:0\", shape=(?, 2048, 64), dtype=float32)\n",
      "output size: 131072 \n",
      "\n",
      "encoder_conv_layer_3 conv params =  16512 bnorm params =  256\n",
      "Tensor(\"experiment_-1_1/Relu_3:0\", shape=(?, 2048, 128), dtype=float32)\n",
      "output size: 262144 \n",
      "\n",
      "encoder_conv_layer_4 conv params =  526336 bnorm params =  4096\n",
      "Tensor(\"experiment_-1_1/Relu_4:0\", shape=(?, 2048, 2048), dtype=float32)\n",
      "output size: 4194304 \n",
      "\n",
      "Tensor(\"experiment_-1_1/Max:0\", shape=(?, 2048), dtype=float32)\n",
      "Building Decoder\n",
      "decoder_fc_0 FC params =  65568 bnorm params =  64\n",
      "Tensor(\"experiment_-1_1/Relu_5:0\", shape=(?, 32), dtype=float32)\n",
      "output size: 32 \n",
      "\n",
      "decoder_fc_1 FC params =  8448 bnorm params =  512\n",
      "Tensor(\"experiment_-1_1/Relu_6:0\", shape=(?, 256), dtype=float32)\n",
      "output size: 256 \n",
      "\n",
      "decoder_fc_2 FC params =  1579008 Tensor(\"experiment_-1_1/decoder_fc_2/BiasAdd:0\", shape=(?, 6144), dtype=float32)\n",
      "output size: 6144 \n",
      "\n",
      "2206352\n"
     ]
    }
   ],
   "source": [
    "if load_pretrained_model:\n",
    "    conf = Conf.load(osp.join(train_dir, 'configuration'))\n",
    "    print conf\n",
    "    if conf.train_dir != train_dir:\n",
    "        conf.train_dir = train_dir\n",
    "#     conf.save(osp.join(train_dir, 'configuration'))\n",
    "else:    \n",
    "    conf = Conf(\n",
    "                n_input = [n_pc_points, 3],\n",
    "                loss = loss,\n",
    "                training_epochs = training_epochs,\n",
    "                batch_size = batch_size,\n",
    "                denoising = False,\n",
    "                learning_rate = learning_rate,\n",
    "                train_dir = train_dir,\n",
    "                loss_display_step = 1,\n",
    "                saver_step = None,\n",
    "                z_rotate = False,\n",
    "                encoder = encoder,\n",
    "                decoder = decoder,\n",
    "                encoder_args = encoder_args,\n",
    "                decoder_args = decoder_args\n",
    "               )\n",
    "\n",
    "conf.experiment_name = 'experiment_' + str(exp_id)\n",
    "conf.held_out_step = 5\n",
    "reset_tf_graph()\n",
    "ae = PointNetAutoEncoder(conf.experiment_name, conf)\n",
    "print ae.trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', '0001', 'training time (minutes)=', '0.8430', 'loss=', '0.004849095')\n",
      "('Epoch:', '0002', 'training time (minutes)=', '0.7724', 'loss=', '0.002718080')\n",
      "('Epoch:', '0003', 'training time (minutes)=', '0.7672', 'loss=', '0.002391661')\n",
      "('Epoch:', '0004', 'training time (minutes)=', '0.7755', 'loss=', '0.002173143')\n",
      "('Epoch:', '0005', 'training time (minutes)=', '0.7758', 'loss=', '0.002036879')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0211', 'loss=', '0.001973703')\n",
      "('Epoch:', '0006', 'training time (minutes)=', '0.7755', 'loss=', '0.001930233')\n",
      "('Epoch:', '0007', 'training time (minutes)=', '0.7669', 'loss=', '0.001850985')\n",
      "('Epoch:', '0008', 'training time (minutes)=', '0.7765', 'loss=', '0.001768912')\n",
      "('Epoch:', '0009', 'training time (minutes)=', '0.7667', 'loss=', '0.001731401')\n",
      "('Epoch:', '0010', 'training time (minutes)=', '0.7803', 'loss=', '0.001669446')\n",
      "('Held Out Data :', 'forward time (minutes)=', '0.0203', 'loss=', '0.001731193')\n",
      "('Epoch:', '0011', 'training time (minutes)=', '0.7624', 'loss=', '0.001627317')\n",
      "('Epoch:', '0012', 'training time (minutes)=', '0.7744', 'loss=', '0.001582366')\n",
      "('Epoch:', '0013', 'training time (minutes)=', '0.7645', 'loss=', '0.001562168')\n"
     ]
    }
   ],
   "source": [
    "if do_training:\n",
    "    buf_size = 1 # flush each line\n",
    "    fout = open(osp.join(conf.train_dir, 'train_stats.txt'), 'a', buf_size)     \n",
    "    train_stats = ae.train(in_data['train'], conf, log_file=fout, held_out_data=in_data['test'])\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000749164663748 0.000654067897925\n"
     ]
    }
   ],
   "source": [
    "from tf_lab.point_clouds.convenience import reconstruct_pclouds\n",
    "_, l_test = reconstruct_pclouds(ae, in_data['test'].point_clouds, batch_size=100)\n",
    "_, l_train = reconstruct_pclouds(ae, in_data['train'].point_clouds, batch_size=100)\n",
    "print l_test, l_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "a = np.arange(16).reshape(1,4,4)\n",
    "a.repeat(3, axis=2)\n",
    "a = np.arange(16).reshape(1,4,4)\n",
    "print a\n",
    "a = a.reshape(-1)\n",
    "a = np.tile(a, 3)\n",
    "# print a\n",
    "a = np.transpose(a.reshape(1, 3, 4, 4), [0,3,2,1])\n",
    "print a\n",
    "print a.shape\n",
    "\n",
    "n_filters = 5\n",
    "n_pc_points = 4\n",
    "# group_size = 20\n",
    "batch_size = 1\n",
    "\n",
    "in_layer = tf.placeholder(tf.float32, [None, n_pc_points, 3])\n",
    "\n",
    "filter_weights = tf.nn.sigmoid(conv_1d(in_layer, group_size, n_filters))\n",
    "\n",
    "tf.while_loop()\n",
    "\n",
    "\n",
    "# tf.while_loop\n",
    "\n",
    "\n",
    "# filter_weights = tf.constant(np.arange(n_pc_points * n_filters, dtype='f4').reshape(batch_size, n_pc_points, n_filters))\n",
    "\n",
    "# print sess.run(filter_weights)\n",
    "\n",
    "# filter_weights = tf.reshape(filter_weights, [-1, n_pc_points*n_filters])\n",
    "\n",
    "# filter_weights = tf.tile(filter_weights, [1,3]) # expand for x-y-z\n",
    "\n",
    "# filter_weights = tf.transpose(tf.reshape(filter_weights, (1, n_pc_points, n_filters, 3) ), [0, 2, 3, 1])\n",
    "\n",
    "# sess.run(filter_weights[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
